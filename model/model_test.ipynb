{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 설치"
      ],
      "metadata": {
        "id": "0EGVSgLnMDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG8EnQP5dKH4",
        "outputId": "f38a601c-b514-48e6-dfbc-1d8eb55bc822"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 호출"
      ],
      "metadata": {
        "id": "ktZz2tUjMQE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_project_kogpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_project_kogpt2\")"
      ],
      "metadata": {
        "id": "cu-VSwoVL7hK"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 삼행시 함수"
      ],
      "metadata": {
        "id": "N8ZU3pQpMrz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 사용 여부\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4esBql1YQNB",
        "outputId": "107627d1-d9a2-4cab-e039-ab5f8669b561"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mind(input_letter):\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        times = 0\n",
        "        while times < 3:\n",
        "            # 만약 idx 가 0 이라면 == 첫 글자\n",
        "            if idx == 0:\n",
        "                # 첫 글자 인코딩\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                \n",
        "                # 첫 글자 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "            \n",
        "            # 첫 글자가 아니라면\n",
        "            else:\n",
        "                # 좀더 매끄러운 삼행시를 위해 이전 문장이랑 현재 음절 연결\n",
        "                # 이후 generate 된 문장에서 이전 문장에 대한 데이터 제거\n",
        "                link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "                # print(link_with_pre_sentence)\n",
        "\n",
        "                # 연결된 문장을 인코딩\n",
        "                input_ids = tokenizer.encode(\n",
        "                link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                print(input_ids)\n",
        "                # 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=len_sequence + 5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "\n",
        "            # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "            generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "            # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            # 첫 글자가 아니라면, generate 된 음절만 결과물 list에 들어갈 수 있게 앞 문장에 대한 인코딩 값 제거\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # 이전 문장의 길이 이후로 슬라이싱해서 앞 문장 제거\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "                # 다음 음절을 위해 길이 갱신\n",
        "                len_sequence += len(generated_sequence)        \n",
        "            \n",
        "            # 첫 글자라면\n",
        "            else:\n",
        "                # 시퀀스 길이 저장\n",
        "                len_sequence = len(generated_sequence)\n",
        "\n",
        "            # 결과물 디코딩\n",
        "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "            if len(decoded_sequence) > 1:\n",
        "                break\n",
        "            else:\n",
        "                times += 1\n",
        "                continue\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "        # print(res_l)\n",
        "\n",
        "    # 결과물 list에서 한 줄씩 출력\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        print(f\"{letter} :\", res)"
      ],
      "metadata": {
        "id": "UJeuJqzOSNKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mind2(input_letter):\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "\n",
        "        while True:\n",
        "            # 만약 idx 가 0 이라면 == 첫 글자\n",
        "            if idx == 0:\n",
        "                # 첫 글자 인코딩\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                # print(f\"{idx}번 인코딩 : {input_ids}\\n\") # 2차원 텐서\n",
        "\n",
        "                # 첫 글자 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)[0]\n",
        "                # print(\"첫 글자 인코딩 후 generate 결과:\", output_sequence, \"\\n\") # tensor\n",
        "\n",
        "            # 첫 글자가 아니라면\n",
        "            else:\n",
        "                # 한 음절\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                # print(f\"{idx}번 째 글자 인코딩 : {input_ids} \\n\")\n",
        "\n",
        "                # 좀더 매끄러운 삼행시를 위해 이전 인코딩과 지금 인코딩 연결\n",
        "                link_with_pre_sentence = torch.cat((generated_sequence, input_ids[0]), 0)\n",
        "                link_with_pre_sentence = torch.reshape(link_with_pre_sentence, (1, len(link_with_pre_sentence)))\n",
        "                # print(f\"이전 텐서와 연결된 텐서 {link_with_pre_sentence} \\n\")\n",
        "\n",
        "                # 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=link_with_pre_sentence.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)[0]\n",
        "                # print(f\"{idx}번 인코딩 후 generate : {output_sequence}\")\n",
        "        \n",
        "            # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "            generated_sequence = output_sequence.tolist()\n",
        "            # print(f\"{idx}번 인코딩 리스트 : {generated_sequence} \\n\")\n",
        "\n",
        "            # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            generated_sequence = torch.tensor(generated_sequence) \n",
        "            # print(f\"{idx}번 인코딩 리스트 패딩 제거 후 다시 텐서 : {generated_sequence} \\n\")\n",
        "\n",
        "            # 첫 글자가 아니라면, generate 된 음절만 결과물 list에 들어갈 수 있게 앞 문장에 대한 인코딩 값 제거\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # 이전 문장의 길이 이후로 슬라이싱해서 앞 문장 제거\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            len_sequence = len(generated_sequence)\n",
        "            # print(\"len_seq\", len_sequence)\n",
        "\n",
        "            # 음절 그대로 뱉으면 다시 해와, 아니면 while문 탈출\n",
        "            if len_sequence > 1:\n",
        "                break\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(generated_sequence)\n",
        "\n",
        "        # print(\"res_l :\", res_l)\n",
        "\n",
        "    # 결과물 list에서 한 줄씩 출력\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        decode_res = tokenizer.decode(res, clean_up_tokenization_spaces=True)\n",
        "        print(f\"{letter} :\", decode_res)"
      ],
      "metadata": {
        "id": "-hir558Pz1S_"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mind2(\"어어어\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz_CjqVUtqCM",
        "outputId": "95a69fd7-ff0c-47ff-b789-fa5ae3d05c30"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어 : 어서 와 내게 내 안에 숨겨둔 이게 사랑이라면\n",
            "어 : 어떨까\n",
            "어 : 어떡할까\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind2(\"고고고\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHiL61212VQ",
        "outputId": "b5ce1e95-0235-4e8b-d469-e2dfa4091d97"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "고 : 고맙다는 말도 하지 못하고\n",
            "고 : 고마워서\n",
            "고 : 고맙단 말도 못 했어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"해파리\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SRF781-kFoc",
        "outputId": "5fe76a63-79cd-4186-fc48-5020d0916963"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "해 : 해줄게 없기에\n",
            "파 : 파 무색해\n",
            "리 : 이토록 예쁠까\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"아이스크림\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdHiRak_iKk5",
        "outputId": "eff02ab2-514b-4747-b13e-d83c79189aed"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아 : 아련히 들리네 눈 내리던 어느 날\n",
            "이 : 이 거리에 낯선\n",
            "스 : 스쳐 지나고\n",
            "크 : 크면 잊혀질까요\n",
            "림 : 임\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"파이널\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1WvJYUYGH8",
        "outputId": "c90e48cc-045d-4a6d-94f7-7321ed230eb5"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파 : 파랑새 울어요\n",
            "이 : 이 마음\n",
            "널 : 널 향하는 내 맘 받아줘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"박경택\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6RlCBG0hYE",
        "outputId": "1426f62d-57ba-4848-c9c0-9ef90dbb8504"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "박 : 박혀버린 깨진 꿈 조각\n",
            "경 : 경적\n",
            "택 : 택이\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"문종현\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaVSRcL2YIW0",
        "outputId": "7e54a274-add1-4020-b53d-52aa4841e33b"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문 : 문득 생각이 난 거야\n",
            "종 : 종잡치 못한\n",
            "현 : 현길 홀로\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"이정은\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9TsrSVZabb8",
        "outputId": "36443d1e-ad21-4d0a-8650-562210f43d1d"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 : 이젠 내게서 떠나 가라고\n",
            "정 : 정말로 괜찮나요\n",
            "은 : 은마오\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"권소희\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl9KQ8xRy_PD",
        "outputId": "32504206-048b-455c-dd80-7b31c2cea54d"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "권 : 권태기란 말은 다\n",
            "소 : 소용이 없어\n",
            "희 : 희미한 시계소리만\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"박건영\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc2DtSbVcK9-",
        "outputId": "a93ec931-e801-4fdf-bbeb-4023ddbd27e3"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "박 : 박수를 치자\n",
            "건 : 건네주겠지\n",
            "영 : 영영 떠나갈꺼야\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"정재영\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PItWGW0xcQFN",
        "outputId": "eb7a1ae5-876d-4b07-d544-a031be6a55cf"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정 : 정말로 괜찮다니\n",
            "재 : 재미가 없어\n",
            "영 : 영영\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"구자현\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCE22o8bzhbM",
        "outputId": "5652a0a9-932c-4d52-886c-0183d48a1be9"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "구 : 구겨진 내 마음 알 때도\n",
            "자 : 자꾸만\n",
            "현 : 현상에 겨워\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"김의준\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfiH0W_K1YUQ",
        "outputId": "a4d3b6c4-3cfe-4a7c-f85c-d83c1b8477a9"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김 : 김건모 노래\n",
            "의 : 의이지만\n",
            "준 : 준 것 없는\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"최지영\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuDuEWtscqwG",
        "outputId": "904acfc5-5cc4-47a2-f4e1-0723a0a496dd"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최 : 최선이라고 생각했어\n",
            "지 : 지치고 힘이 들 때\n",
            "영 : 영원을 바래\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"이지혜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I69iP1ombVg",
        "outputId": "5b0c9445-47d7-408a-f2de-a36471ab6a49"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 : 이젠 그만 너를 보내야 해\n",
            "지 : 지겹게도 놓여진\n",
            "혜 : 혜화원 아저씨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 혹시 몰라 저장\n",
        "* 와일문 추가 전 스트림릿"
      ],
      "metadata": {
        "id": "7gfY8cxwBPsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import streamlit as st\n",
        "from streamlit_lottie import st_lottie\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Page Config\n",
        "st.set_page_config(\n",
        "    page_title=\"노래 가사 n행시\",\n",
        "    page_icon=\"💌\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "### Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_20man\")\n",
        "\n",
        "@st.cache(show_spinner=False)\n",
        "def load_model():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_20man\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Class : Dict 중복 키 출력\n",
        "class poem(object):\n",
        "    def __init__(self,letter):\n",
        "        self.letter = letter\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.letter\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"'\"+self.letter+\"'\"\n",
        "\n",
        "\n",
        "def n_line_poem(input_letter):\n",
        "\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        # 만약 idx 가 0 이라면 == 첫 글자\n",
        "        if idx == 0:\n",
        "            # 첫 글자 인코딩\n",
        "            input_ids = tokenizer.encode(\n",
        "            val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            \n",
        "            # 첫 글자 인코딩 값으로 문장 생성\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=5, temperature=0.9, repetition_penalty=1.5)\n",
        "        \n",
        "        # 첫 글자가 아니라면\n",
        "        else:\n",
        "            # 좀더 매끄러운 삼행시를 위해 이전 문장이랑 현재 음절 연결\n",
        "            # 이후 generate 된 문장에서 이전 문장에 대한 데이터 제거\n",
        "            link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "            # print(link_with_pre_sentence)\n",
        "\n",
        "            # 연결된 문장을 인코딩\n",
        "            input_ids = tokenizer.encode(\n",
        "            link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "            # 인코딩 값으로 문장 생성\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=len_sequence, temperature=0.9, repetition_penalty=1.5)\n",
        "\n",
        "        # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "        generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "        # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "        if tokenizer.pad_token_id in generated_sequence:\n",
        "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "        \n",
        "        # 첫 글자가 아니라면, generate 된 음절만 결과물 list에 들어갈 수 있게 앞 문장에 대한 인코딩 값 제거\n",
        "        # print(generated_sequence)\n",
        "        if idx != 0:\n",
        "            # 이전 문장의 길이 이후로 슬라이싱해서 앞 문장 제거\n",
        "            generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            # 다음 음절을 위해 길이 갱신\n",
        "            len_sequence += len(generated_sequence)        \n",
        "        \n",
        "        # 첫 글자라면\n",
        "        else:\n",
        "            # 시퀀스 길이 저장\n",
        "            len_sequence = len(generated_sequence)\n",
        "        \n",
        "        # print(last_sequence)\n",
        "\n",
        "        # 결과물 디코딩\n",
        "        decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "    poem_dict = {}\n",
        "\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        poem_dict[poem(letter)] = res\n",
        "\n",
        "    return poem_dict\n",
        "\n",
        "###\n",
        "\n",
        "# Image(.gif)\n",
        "@st.cache(show_spinner=False)\n",
        "def load_lottieurl(url: str):\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "lottie_url = \"https://assets7.lottiefiles.com/private_files/lf30_fjln45y5.json\"\n",
        "\n",
        "lottie_json = load_lottieurl(lottie_url)\n",
        "st_lottie(lottie_json, speed=1, height=200, key=\"initial\")\n",
        "\n",
        "\n",
        "# Title\n",
        "row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.columns(\n",
        "    (0.01, 2, 0.05, 1, 0.01)\n",
        ")\n",
        "\n",
        "with row0_1:\n",
        "    st.title(\"한글 노래 가사 n행시\")\n",
        "    st.subheader(\"멋쟁이사자처럼 AIS7 파이널 프로젝트\")\n",
        "\n",
        "with row0_2:\n",
        "    st.write(\"\")\n",
        "    st.subheader(\n",
        "        \"해파리팀\"\n",
        "    )\n",
        "    st.write(\"이지혜, 최지영, 권소희\")\n",
        "    st.write(\"문종현, 구자현, 김의준\")\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Explanation\n",
        "row1_spacer1, row1_1, row1_spacer2 = st.columns((0.01, 3, 0.01))\n",
        "\n",
        "with row1_1:\n",
        "    st.markdown(\n",
        "        \"**'MZ세대'에게**\"\n",
        "    )\n",
        "    st.markdown(\n",
        "        \"음악은 세대를 드러내는 지표이자 자신의 감정 및 공동체를 드러내는 수단이다.\"\n",
        "    )\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Model & Input\n",
        "row2_spacer1, row2_1, row2_spacer2 = st.columns((0.01, 1.5, 0.05))\n",
        "\n",
        "# Word Input\n",
        "if \"generate\" not in st.session_state:\n",
        "    st.session_state.generate = False\n",
        "\n",
        "with row2_1:\n",
        "    word_input = st.text_input(\n",
        "            \"n행시에 사용할 단어를 적고 Enter를 눌러주세요. 👇\",\n",
        "            placeholder='한글 단어',\n",
        "            max_chars=10\n",
        "    )\n",
        "    \n",
        "    if word_input:\n",
        "        st.write(\"n행시 단어 :  \", word_input)\n",
        "\n",
        "    if st.button('n행시 제작하기'):\n",
        "        with st.spinner('잠시 기다려주세요...'):\n",
        "            result = n_line_poem(word_input)\n",
        "        st.success('완료됐습니다!')\n",
        "        for r in result:\n",
        "            st.write(f'{r} : {result[r]}')"
      ],
      "metadata": {
        "id": "msfSlTM9kyXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor, cat, reshape\n",
        "t1 = tensor([[ 9208,  6866, 12422, 31656,  7285,  9065]])\n",
        "t2 = tensor([[ 9208,  6866, 12422, 31656,  7285,  9111]])\n",
        "\n",
        "t = cat((t1[0], t2[0]), 0)\n",
        "print(t)\n",
        "print(len(t))\n",
        "reshape(t, (1, 12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WdxvVZzeH3U",
        "outputId": "f32029bd-b4a3-4e51-87de-7c11afc8eb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 9208,  6866, 12422, 31656,  7285,  9065,  9208,  6866, 12422, 31656,\n",
            "         7285,  9111])\n",
            "12\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9208,  6866, 12422, 31656,  7285,  9065,  9208,  6866, 12422, 31656,\n",
              "          7285,  9111]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    }
  ]
}