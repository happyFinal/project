{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 설치"
      ],
      "metadata": {
        "id": "0EGVSgLnMDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG8EnQP5dKH4",
        "outputId": "b7a75837-23ce-40fb-94c0-1de3b960b0bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 호출"
      ],
      "metadata": {
        "id": "ktZz2tUjMQE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_project_kogpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_project_kogpt2\")"
      ],
      "metadata": {
        "id": "cu-VSwoVL7hK"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 삼행시 함수"
      ],
      "metadata": {
        "id": "N8ZU3pQpMrz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU 사용 여부\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4esBql1YQNB",
        "outputId": "b76933ba-2e33-4139-bb50-e9428acedc80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mind(input_letter):\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        times = 0\n",
        "        while times < 3:\n",
        "            # 만약 idx 가 0 이라면 == 첫 글자\n",
        "            if idx == 0:\n",
        "                # 첫 글자 인코딩\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                \n",
        "                # 첫 글자 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "            \n",
        "            # 첫 글자가 아니라면\n",
        "            else:\n",
        "                # 좀더 매끄러운 삼행시를 위해 이전 문장이랑 현재 음절 연결\n",
        "                # 이후 generate 된 문장에서 이전 문장에 대한 데이터 제거\n",
        "                link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "                # print(link_with_pre_sentence)\n",
        "\n",
        "                # 연결된 문장을 인코딩\n",
        "                input_ids = tokenizer.encode(\n",
        "                link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "                # 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=len_sequence + 5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "\n",
        "            # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "            generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "            # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            # 첫 글자가 아니라면, generate 된 음절만 결과물 list에 들어갈 수 있게 앞 문장에 대한 인코딩 값 제거\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # 이전 문장의 길이 이후로 슬라이싱해서 앞 문장 제거\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "                # 다음 음절을 위해 길이 갱신\n",
        "                len_sequence += len(generated_sequence)        \n",
        "            \n",
        "            # 첫 글자라면\n",
        "            else:\n",
        "                # 시퀀스 길이 저장\n",
        "                len_sequence = len(generated_sequence)\n",
        "\n",
        "            # 결과물 디코딩\n",
        "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "            if len(decoded_sequence) > 1:\n",
        "                break\n",
        "            else:\n",
        "                times += 1\n",
        "                continue\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "        # print(res_l)\n",
        "\n",
        "    # 결과물 list에서 한 줄씩 출력\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        print(f\"{letter} :\", res)"
      ],
      "metadata": {
        "id": "zZgiBQRtYDqc"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"배고파\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPh0aX8gtnXt",
        "outputId": "c2773fdd-ae90-4949-d96c-f0ba181fa983"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "배 : 배트맨과 조커\n",
            "고 : 고저 케인들\n",
            "파 : 파랑 던들\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"코코아\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz_CjqVUtqCM",
        "outputId": "4ac151b7-2ada-4552-ed83-8012de124447"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "코 : 코끝이 찡한 것을 느끼지\n",
            "코 : 코 끝이 닿게\n",
            "아 : 아련한 기억\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"해파리\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SRF781-kFoc",
        "outputId": "9597a1a0-acb8-48d3-fabb-e69ac14f9ead"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "해 : 해맑게 웃어주는 너\n",
            "파 : 파노을처럼\n",
            "리 : 이 길을 달려\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"아이스크림\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdHiRak_iKk5",
        "outputId": "246341af-d7a9-4a82-9f2a-74c52d4c413e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "아 : 아련하게 속삭이는\n",
            "이 : 이 한마디\n",
            "스 : 스쳐가는 인연이길\n",
            "크 : 크겠어요\n",
            "림 : 임을놓지 않는 아름다운 그대 그리운 말들은 사라져 가네 여기 서있는데 절대 뒤돌아보지마 하지만\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"파이널\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1WvJYUYGH8",
        "outputId": "6c5afaee-5e43-4b9c-e5d9-7a87be1066ba"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파 : 파도에 사라져\n",
            "이 : 이젠\n",
            "널 : 널 잊어보려고 해\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"박경택\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6RlCBG0hYE",
        "outputId": "beb5af6e-a111-4882-c461-44535990e81c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "박 : 박혀있는 그 조각들을\n",
            "경 : 경적해\n",
            "택 : 택시 끝났어 첨으로 눈을 뗄 수 없었지\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"문종현\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaVSRcL2YIW0",
        "outputId": "bb9ff9d0-3af1-47df-ca7e-08b9a7a60f06"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문 : 문득 그리운 맘에 전화를 걸죠\n",
            "종 : 종일\n",
            "현 : 현잔한 기분오던\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"이정은\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9TsrSVZabb8",
        "outputId": "840b358c-127a-4c94-877a-24a77f1d8c66"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 : 이 밤이 새도록 널 기다려\n",
            "정 : 정말이야\n",
            "은 : 은빛은 우릴 돌려주네\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"권소희\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl9KQ8xRy_PD",
        "outputId": "52d10e88-a4ff-4836-9997-d9baaeaddfc3"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "권 : 권태기가 아냐 이건 끝 대체 어디부터 잘못된건지\n",
            "소 : 소용이 없어\n",
            "희 : 희미한 불빛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"박건영\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc2DtSbVcK9-",
        "outputId": "a5f2ac40-c6d9-4aa6-b10a-ae708eef91c3"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "박 : 박수를 치자\n",
            "건 : 건네주리\n",
            "영 : 영영 차가운 시선\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"정재영\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PItWGW0xcQFN",
        "outputId": "983a9e1e-2a98-4454-e769-471f51d44aa6"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정 : 정처 없이 걷다가\n",
            "재 : 재촉하는 낯선 발걸음은\n",
            "영 : 영원이게 멀어지네\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"구자현\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCE22o8bzhbM",
        "outputId": "8473bd79-63cd-4cf3-b4bf-e8bc2d0d7cc4"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "구 : 구겨져진 지난 기억\n",
            "자 : 자꾸만\n",
            "현 : 현아\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"김의준\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfiH0W_K1YUQ",
        "outputId": "f26abb38-0b7a-4998-85c8-1a3c313e1a63"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "김 : 김건모 노래\n",
            "의 : 의자와\n",
            "준 : 준담\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"최지영\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuDuEWtscqwG",
        "outputId": "94613f2b-c818-484c-9795-055332646c80"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최 : 최희 입니다\n",
            "지 : 지친 맘을\n",
            "영 : 영영 닫혀진 문 틈에서 난\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"이지혜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I69iP1ombVg",
        "outputId": "440b60f2-78b7-4763-ef30-e120f0a3433c"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 : 이제와서 비워야하니 자신이 없어\n",
            "지 : 지우려 해도 만질 수 없는 걸\n",
            "혜 : 혜오\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 혹시 몰라 저장\n",
        "* 와일문 추가 전 스트림릿"
      ],
      "metadata": {
        "id": "7gfY8cxwBPsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import streamlit as st\n",
        "from streamlit_lottie import st_lottie\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Page Config\n",
        "st.set_page_config(\n",
        "    page_title=\"노래 가사 n행시\",\n",
        "    page_icon=\"💌\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "### Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_20man\")\n",
        "\n",
        "@st.cache(show_spinner=False)\n",
        "def load_model():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_20man\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Class : Dict 중복 키 출력\n",
        "class poem(object):\n",
        "    def __init__(self,letter):\n",
        "        self.letter = letter\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.letter\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"'\"+self.letter+\"'\"\n",
        "\n",
        "\n",
        "def n_line_poem(input_letter):\n",
        "\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        # 만약 idx 가 0 이라면 == 첫 글자\n",
        "        if idx == 0:\n",
        "            # 첫 글자 인코딩\n",
        "            input_ids = tokenizer.encode(\n",
        "            val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            \n",
        "            # 첫 글자 인코딩 값으로 문장 생성\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=5, temperature=0.9, repetition_penalty=1.5)\n",
        "        \n",
        "        # 첫 글자가 아니라면\n",
        "        else:\n",
        "            # 좀더 매끄러운 삼행시를 위해 이전 문장이랑 현재 음절 연결\n",
        "            # 이후 generate 된 문장에서 이전 문장에 대한 데이터 제거\n",
        "            link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "            # print(link_with_pre_sentence)\n",
        "\n",
        "            # 연결된 문장을 인코딩\n",
        "            input_ids = tokenizer.encode(\n",
        "            link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "            # 인코딩 값으로 문장 생성\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=len_sequence, temperature=0.9, repetition_penalty=1.5)\n",
        "\n",
        "        # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "        generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "        # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "        if tokenizer.pad_token_id in generated_sequence:\n",
        "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "        \n",
        "        # 첫 글자가 아니라면, generate 된 음절만 결과물 list에 들어갈 수 있게 앞 문장에 대한 인코딩 값 제거\n",
        "        # print(generated_sequence)\n",
        "        if idx != 0:\n",
        "            # 이전 문장의 길이 이후로 슬라이싱해서 앞 문장 제거\n",
        "            generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            # 다음 음절을 위해 길이 갱신\n",
        "            len_sequence += len(generated_sequence)        \n",
        "        \n",
        "        # 첫 글자라면\n",
        "        else:\n",
        "            # 시퀀스 길이 저장\n",
        "            len_sequence = len(generated_sequence)\n",
        "        \n",
        "        # print(last_sequence)\n",
        "\n",
        "        # 결과물 디코딩\n",
        "        decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "    poem_dict = {}\n",
        "\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        poem_dict[poem(letter)] = res\n",
        "\n",
        "    return poem_dict\n",
        "\n",
        "###\n",
        "\n",
        "# Image(.gif)\n",
        "@st.cache(show_spinner=False)\n",
        "def load_lottieurl(url: str):\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "lottie_url = \"https://assets7.lottiefiles.com/private_files/lf30_fjln45y5.json\"\n",
        "\n",
        "lottie_json = load_lottieurl(lottie_url)\n",
        "st_lottie(lottie_json, speed=1, height=200, key=\"initial\")\n",
        "\n",
        "\n",
        "# Title\n",
        "row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.columns(\n",
        "    (0.01, 2, 0.05, 1, 0.01)\n",
        ")\n",
        "\n",
        "with row0_1:\n",
        "    st.title(\"한글 노래 가사 n행시\")\n",
        "    st.subheader(\"멋쟁이사자처럼 AIS7 파이널 프로젝트\")\n",
        "\n",
        "with row0_2:\n",
        "    st.write(\"\")\n",
        "    st.subheader(\n",
        "        \"해파리팀\"\n",
        "    )\n",
        "    st.write(\"이지혜, 최지영, 권소희\")\n",
        "    st.write(\"문종현, 구자현, 김의준\")\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Explanation\n",
        "row1_spacer1, row1_1, row1_spacer2 = st.columns((0.01, 3, 0.01))\n",
        "\n",
        "with row1_1:\n",
        "    st.markdown(\n",
        "        \"**'MZ세대'에게**\"\n",
        "    )\n",
        "    st.markdown(\n",
        "        \"음악은 세대를 드러내는 지표이자 자신의 감정 및 공동체를 드러내는 수단이다.\"\n",
        "    )\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Model & Input\n",
        "row2_spacer1, row2_1, row2_spacer2 = st.columns((0.01, 1.5, 0.05))\n",
        "\n",
        "# Word Input\n",
        "if \"generate\" not in st.session_state:\n",
        "    st.session_state.generate = False\n",
        "\n",
        "with row2_1:\n",
        "    word_input = st.text_input(\n",
        "            \"n행시에 사용할 단어를 적고 Enter를 눌러주세요. 👇\",\n",
        "            placeholder='한글 단어',\n",
        "            max_chars=10\n",
        "    )\n",
        "    \n",
        "    if word_input:\n",
        "        st.write(\"n행시 단어 :  \", word_input)\n",
        "\n",
        "    if st.button('n행시 제작하기'):\n",
        "        with st.spinner('잠시 기다려주세요...'):\n",
        "            result = n_line_poem(word_input)\n",
        "        st.success('완료됐습니다!')\n",
        "        for r in result:\n",
        "            st.write(f'{r} : {result[r]}')"
      ],
      "metadata": {
        "id": "msfSlTM9kyXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}