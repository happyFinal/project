{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "0EGVSgLnMDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG8EnQP5dKH4",
        "outputId": "f38a601c-b514-48e6-dfbc-1d8eb55bc822"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ í˜¸ì¶œ"
      ],
      "metadata": {
        "id": "ktZz2tUjMQE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_project_kogpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_project_kogpt2\")"
      ],
      "metadata": {
        "id": "cu-VSwoVL7hK"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì‚¼í–‰ì‹œ í•¨ìˆ˜"
      ],
      "metadata": {
        "id": "N8ZU3pQpMrz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU ì‚¬ìš© ì—¬ë¶€\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4esBql1YQNB",
        "outputId": "107627d1-d9a2-4cab-e039-ab5f8669b561"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mind(input_letter):\n",
        "    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „\n",
        "    dooeum = {\"ë¼\":\"ë‚˜\", \"ë½\":\"ë‚™\", \"ë€\":\"ë‚œ\", \"ë„\":\"ë‚ \", \"ëŒ\":\"ë‚¨\", \"ë\":\"ë‚©\", \"ë‘\":\"ë‚­\", \n",
        "          \"ë˜\":\"ë‚´\", \"ë­\":\"ëƒ‰\", \"ëƒ‘\":\"ì•½\", \"ëµ\":\"ì•½\", \"ëƒ¥\":\"ì–‘\", \"ëŸ‰\":\"ì–‘\", \"ë…€\":\"ì—¬\", \n",
        "          \"ë ¤\":\"ì—¬\", \"ë…\":\"ì—­\", \"ë ¥\":\"ì—­\", \"ë…„\":\"ì—°\", \"ë ¨\":\"ì—°\", \"ë…ˆ\":\"ì—´\", \"ë ¬\":\"ì—´\", \n",
        "          \"ë…\":\"ì—¼\", \"ë ´\":\"ì—¼\", \"ë µ\":\"ì—½\", \"ë…•\":\"ì˜\", \"ë ¹\":\"ì˜\", \"ë…œ\":\"ì˜ˆ\", \"ë¡€\":\"ì˜ˆ\", \n",
        "          \"ë¡œ\":\"ë…¸\", \"ë¡\":\"ë…¹\", \"ë¡ \":\"ë…¼\", \"ë¡±\":\"ë†\", \"ë¢°\":\"ë‡Œ\", \"ë‡¨\":\"ìš”\", \"ë£Œ\":\"ìš”\", \n",
        "          \"ë£¡\":\"ìš©\", \"ë£¨\":\"ëˆ„\", \"ë‰´\":\"ìœ \", \"ë¥˜\":\"ìœ \", \"ë‰µ\":\"ìœ¡\", \"ë¥™\":\"ìœ¡\", \"ë¥œ\":\"ìœ¤\", \n",
        "          \"ë¥ \":\"ìœ¨\", \"ë¥­\":\"ìœµ\", \"ë¥µ\":\"ëŠ‘\", \"ë¦„\":\"ëŠ \", \"ë¦‰\":\"ëŠ¥\", \"ë‹ˆ\":\"ì´\", \"ë¦¬\":\"ì´\", \n",
        "          \"ë¦°\":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}\n",
        "    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list\n",
        "    res_l = []\n",
        "\n",
        "    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # ë‘ìŒ ë²•ì¹™ ì ìš©\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        times = 0\n",
        "        while times < 3:\n",
        "            # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì\n",
        "            if idx == 0:\n",
        "                # ì²« ê¸€ì ì¸ì½”ë”©\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                \n",
        "                # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "            \n",
        "            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´\n",
        "            else:\n",
        "                # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ë¬¸ì¥ì´ë‘ í˜„ì¬ ìŒì ˆ ì—°ê²°\n",
        "                # ì´í›„ generate ëœ ë¬¸ì¥ì—ì„œ ì´ì „ ë¬¸ì¥ì— ëŒ€í•œ ë°ì´í„° ì œê±°\n",
        "                link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "                # print(link_with_pre_sentence)\n",
        "\n",
        "                # ì—°ê²°ëœ ë¬¸ì¥ì„ ì¸ì½”ë”©\n",
        "                input_ids = tokenizer.encode(\n",
        "                link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                print(input_ids)\n",
        "                # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=len_sequence + 5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "\n",
        "            # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)\n",
        "            generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "            # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "                # ë‹¤ìŒ ìŒì ˆì„ ìœ„í•´ ê¸¸ì´ ê°±ì‹ \n",
        "                len_sequence += len(generated_sequence)        \n",
        "            \n",
        "            # ì²« ê¸€ìë¼ë©´\n",
        "            else:\n",
        "                # ì‹œí€€ìŠ¤ ê¸¸ì´ ì €ì¥\n",
        "                len_sequence = len(generated_sequence)\n",
        "\n",
        "            # ê²°ê³¼ë¬¼ ë””ì½”ë”©\n",
        "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "            if len(decoded_sequence) > 1:\n",
        "                break\n",
        "            else:\n",
        "                times += 1\n",
        "                continue\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "        # print(res_l)\n",
        "\n",
        "    # ê²°ê³¼ë¬¼ listì—ì„œ í•œ ì¤„ì”© ì¶œë ¥\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        print(f\"{letter} :\", res)"
      ],
      "metadata": {
        "id": "UJeuJqzOSNKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mind2(input_letter):\n",
        "    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „\n",
        "    dooeum = {\"ë¼\":\"ë‚˜\", \"ë½\":\"ë‚™\", \"ë€\":\"ë‚œ\", \"ë„\":\"ë‚ \", \"ëŒ\":\"ë‚¨\", \"ë\":\"ë‚©\", \"ë‘\":\"ë‚­\", \n",
        "          \"ë˜\":\"ë‚´\", \"ë­\":\"ëƒ‰\", \"ëƒ‘\":\"ì•½\", \"ëµ\":\"ì•½\", \"ëƒ¥\":\"ì–‘\", \"ëŸ‰\":\"ì–‘\", \"ë…€\":\"ì—¬\", \n",
        "          \"ë ¤\":\"ì—¬\", \"ë…\":\"ì—­\", \"ë ¥\":\"ì—­\", \"ë…„\":\"ì—°\", \"ë ¨\":\"ì—°\", \"ë…ˆ\":\"ì—´\", \"ë ¬\":\"ì—´\", \n",
        "          \"ë…\":\"ì—¼\", \"ë ´\":\"ì—¼\", \"ë µ\":\"ì—½\", \"ë…•\":\"ì˜\", \"ë ¹\":\"ì˜\", \"ë…œ\":\"ì˜ˆ\", \"ë¡€\":\"ì˜ˆ\", \n",
        "          \"ë¡œ\":\"ë…¸\", \"ë¡\":\"ë…¹\", \"ë¡ \":\"ë…¼\", \"ë¡±\":\"ë†\", \"ë¢°\":\"ë‡Œ\", \"ë‡¨\":\"ìš”\", \"ë£Œ\":\"ìš”\", \n",
        "          \"ë£¡\":\"ìš©\", \"ë£¨\":\"ëˆ„\", \"ë‰´\":\"ìœ \", \"ë¥˜\":\"ìœ \", \"ë‰µ\":\"ìœ¡\", \"ë¥™\":\"ìœ¡\", \"ë¥œ\":\"ìœ¤\", \n",
        "          \"ë¥ \":\"ìœ¨\", \"ë¥­\":\"ìœµ\", \"ë¥µ\":\"ëŠ‘\", \"ë¦„\":\"ëŠ \", \"ë¦‰\":\"ëŠ¥\", \"ë‹ˆ\":\"ì´\", \"ë¦¬\":\"ì´\", \n",
        "          \"ë¦°\":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}\n",
        "    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list\n",
        "    res_l = []\n",
        "\n",
        "    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # ë‘ìŒ ë²•ì¹™ ì ìš©\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "\n",
        "        while True:\n",
        "            # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì\n",
        "            if idx == 0:\n",
        "                # ì²« ê¸€ì ì¸ì½”ë”©\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                # print(f\"{idx}ë²ˆ ì¸ì½”ë”© : {input_ids}\\n\") # 2ì°¨ì› í…ì„œ\n",
        "\n",
        "                # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)[0]\n",
        "                # print(\"ì²« ê¸€ì ì¸ì½”ë”© í›„ generate ê²°ê³¼:\", output_sequence, \"\\n\") # tensor\n",
        "\n",
        "            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´\n",
        "            else:\n",
        "                # í•œ ìŒì ˆ\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                # print(f\"{idx}ë²ˆ ì§¸ ê¸€ì ì¸ì½”ë”© : {input_ids} \\n\")\n",
        "\n",
        "                # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ì¸ì½”ë”©ê³¼ ì§€ê¸ˆ ì¸ì½”ë”© ì—°ê²°\n",
        "                link_with_pre_sentence = torch.cat((generated_sequence, input_ids[0]), 0)\n",
        "                link_with_pre_sentence = torch.reshape(link_with_pre_sentence, (1, len(link_with_pre_sentence)))\n",
        "                # print(f\"ì´ì „ í…ì„œì™€ ì—°ê²°ëœ í…ì„œ {link_with_pre_sentence} \\n\")\n",
        "\n",
        "                # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=link_with_pre_sentence.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)[0]\n",
        "                # print(f\"{idx}ë²ˆ ì¸ì½”ë”© í›„ generate : {output_sequence}\")\n",
        "        \n",
        "            # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)\n",
        "            generated_sequence = output_sequence.tolist()\n",
        "            # print(f\"{idx}ë²ˆ ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸ : {generated_sequence} \\n\")\n",
        "\n",
        "            # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            generated_sequence = torch.tensor(generated_sequence) \n",
        "            # print(f\"{idx}ë²ˆ ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸ íŒ¨ë”© ì œê±° í›„ ë‹¤ì‹œ í…ì„œ : {generated_sequence} \\n\")\n",
        "\n",
        "            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            len_sequence = len(generated_sequence)\n",
        "            # print(\"len_seq\", len_sequence)\n",
        "\n",
        "            # ìŒì ˆ ê·¸ëŒ€ë¡œ ë±‰ìœ¼ë©´ ë‹¤ì‹œ í•´ì™€, ì•„ë‹ˆë©´ whileë¬¸ íƒˆì¶œ\n",
        "            if len_sequence > 1:\n",
        "                break\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°\n",
        "        res_l.append(generated_sequence)\n",
        "\n",
        "        # print(\"res_l :\", res_l)\n",
        "\n",
        "    # ê²°ê³¼ë¬¼ listì—ì„œ í•œ ì¤„ì”© ì¶œë ¥\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        decode_res = tokenizer.decode(res, clean_up_tokenization_spaces=True)\n",
        "        print(f\"{letter} :\", decode_res)"
      ],
      "metadata": {
        "id": "-hir558Pz1S_"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mind2(\"ì–´ì–´ì–´\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz_CjqVUtqCM",
        "outputId": "95a69fd7-ff0c-47ff-b789-fa5ae3d05c30"
      },
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì–´ : ì–´ì„œ ì™€ ë‚´ê²Œ ë‚´ ì•ˆì— ìˆ¨ê²¨ë‘” ì´ê²Œ ì‚¬ë‘ì´ë¼ë©´\n",
            "ì–´ : ì–´ë–¨ê¹Œ\n",
            "ì–´ : ì–´ë–¡í• ê¹Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind2(\"ê³ ê³ ê³ \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHiL61212VQ",
        "outputId": "b5ce1e95-0235-4e8b-d469-e2dfa4091d97"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê³  : ê³ ë§™ë‹¤ëŠ” ë§ë„ í•˜ì§€ ëª»í•˜ê³ \n",
            "ê³  : ê³ ë§ˆì›Œì„œ\n",
            "ê³  : ê³ ë§™ë‹¨ ë§ë„ ëª» í–ˆì–´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"í•´íŒŒë¦¬\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SRF781-kFoc",
        "outputId": "5fe76a63-79cd-4186-fc48-5020d0916963"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í•´ : í•´ì¤„ê²Œ ì—†ê¸°ì—\n",
            "íŒŒ : íŒŒ ë¬´ìƒ‰í•´\n",
            "ë¦¬ : ì´í† ë¡ ì˜ˆì ê¹Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì•„ì´ìŠ¤í¬ë¦¼\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdHiRak_iKk5",
        "outputId": "eff02ab2-514b-4747-b13e-d83c79189aed"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•„ : ì•„ë ¨íˆ ë“¤ë¦¬ë„¤ ëˆˆ ë‚´ë¦¬ë˜ ì–´ëŠ ë‚ \n",
            "ì´ : ì´ ê±°ë¦¬ì— ë‚¯ì„ \n",
            "ìŠ¤ : ìŠ¤ì³ ì§€ë‚˜ê³ \n",
            "í¬ : í¬ë©´ ìŠí˜€ì§ˆê¹Œìš”\n",
            "ë¦¼ : ì„\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"íŒŒì´ë„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1WvJYUYGH8",
        "outputId": "c90e48cc-045d-4a6d-94f7-7321ed230eb5"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒŒ : íŒŒë‘ìƒˆ ìš¸ì–´ìš”\n",
            "ì´ : ì´ ë§ˆìŒ\n",
            "ë„ : ë„ í–¥í•˜ëŠ” ë‚´ ë§˜ ë°›ì•„ì¤˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë°•ê²½íƒ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6RlCBG0hYE",
        "outputId": "1426f62d-57ba-4848-c9c0-9ef90dbb8504"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°• : ë°•í˜€ë²„ë¦° ê¹¨ì§„ ê¿ˆ ì¡°ê°\n",
            "ê²½ : ê²½ì \n",
            "íƒ : íƒì´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë¬¸ì¢…í˜„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaVSRcL2YIW0",
        "outputId": "7e54a274-add1-4020-b53d-52aa4841e33b"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ : ë¬¸ë“ ìƒê°ì´ ë‚œ ê±°ì•¼\n",
            "ì¢… : ì¢…ì¡ì¹˜ ëª»í•œ\n",
            "í˜„ : í˜„ê¸¸ í™€ë¡œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì´ì •ì€\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9TsrSVZabb8",
        "outputId": "36443d1e-ad21-4d0a-8650-562210f43d1d"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ : ì´ì   ë‚´ê²Œì„œ ë– ë‚˜ ê°€ë¼ê³ \n",
            "ì • : ì •ë§ë¡œ ê´œì°®ë‚˜ìš”\n",
            "ì€ : ì€ë§ˆì˜¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ê¶Œì†Œí¬\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl9KQ8xRy_PD",
        "outputId": "32504206-048b-455c-dd80-7b31c2cea54d"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê¶Œ : ê¶Œíƒœê¸°ë€ ë§ì€ ë‹¤\n",
            "ì†Œ : ì†Œìš©ì´ ì—†ì–´\n",
            "í¬ : í¬ë¯¸í•œ ì‹œê³„ì†Œë¦¬ë§Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë°•ê±´ì˜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc2DtSbVcK9-",
        "outputId": "a93ec931-e801-4fdf-bbeb-4023ddbd27e3"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°• : ë°•ìˆ˜ë¥¼ ì¹˜ì\n",
            "ê±´ : ê±´ë„¤ì£¼ê² ì§€\n",
            "ì˜ : ì˜ì˜ ë– ë‚˜ê°ˆêº¼ì•¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì •ì¬ì˜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PItWGW0xcQFN",
        "outputId": "eb7a1ae5-876d-4b07-d544-a031be6a55cf"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì • : ì •ë§ë¡œ ê´œì°®ë‹¤ë‹ˆ\n",
            "ì¬ : ì¬ë¯¸ê°€ ì—†ì–´\n",
            "ì˜ : ì˜ì˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"êµ¬ìí˜„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCE22o8bzhbM",
        "outputId": "5652a0a9-932c-4d52-886c-0183d48a1be9"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "êµ¬ : êµ¬ê²¨ì§„ ë‚´ ë§ˆìŒ ì•Œ ë•Œë„\n",
            "ì : ìê¾¸ë§Œ\n",
            "í˜„ : í˜„ìƒì— ê²¨ì›Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ê¹€ì˜ì¤€\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfiH0W_K1YUQ",
        "outputId": "a4d3b6c4-3cfe-4a7c-f85c-d83c1b8477a9"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê¹€ : ê¹€ê±´ëª¨ ë…¸ë˜\n",
            "ì˜ : ì˜ì´ì§€ë§Œ\n",
            "ì¤€ : ì¤€ ê²ƒ ì—†ëŠ”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ìµœì§€ì˜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuDuEWtscqwG",
        "outputId": "904acfc5-5cc4-47a2-f4e1-0723a0a496dd"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœ : ìµœì„ ì´ë¼ê³  ìƒê°í–ˆì–´\n",
            "ì§€ : ì§€ì¹˜ê³  í˜ì´ ë“¤ ë•Œ\n",
            "ì˜ : ì˜ì›ì„ ë°”ë˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì´ì§€í˜œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I69iP1ombVg",
        "outputId": "5b0c9445-47d7-408a-f2de-a36471ab6a49"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ : ì´ì   ê·¸ë§Œ ë„ˆë¥¼ ë³´ë‚´ì•¼ í•´\n",
            "ì§€ : ì§€ê²¹ê²Œë„ ë†“ì—¬ì§„\n",
            "í˜œ : í˜œí™”ì› ì•„ì €ì”¨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í˜¹ì‹œ ëª°ë¼ ì €ì¥\n",
        "* ì™€ì¼ë¬¸ ì¶”ê°€ ì „ ìŠ¤íŠ¸ë¦¼ë¦¿"
      ],
      "metadata": {
        "id": "7gfY8cxwBPsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import streamlit as st\n",
        "from streamlit_lottie import st_lottie\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Page Config\n",
        "st.set_page_config(\n",
        "    page_title=\"ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œ\",\n",
        "    page_icon=\"ğŸ’Œ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "### Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_20man\")\n",
        "\n",
        "@st.cache(show_spinner=False)\n",
        "def load_model():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_20man\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Class : Dict ì¤‘ë³µ í‚¤ ì¶œë ¥\n",
        "class poem(object):\n",
        "    def __init__(self,letter):\n",
        "        self.letter = letter\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.letter\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"'\"+self.letter+\"'\"\n",
        "\n",
        "\n",
        "def n_line_poem(input_letter):\n",
        "\n",
        "    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „\n",
        "    dooeum = {\"ë¼\":\"ë‚˜\", \"ë½\":\"ë‚™\", \"ë€\":\"ë‚œ\", \"ë„\":\"ë‚ \", \"ëŒ\":\"ë‚¨\", \"ë\":\"ë‚©\", \"ë‘\":\"ë‚­\", \n",
        "          \"ë˜\":\"ë‚´\", \"ë­\":\"ëƒ‰\", \"ëƒ‘\":\"ì•½\", \"ëµ\":\"ì•½\", \"ëƒ¥\":\"ì–‘\", \"ëŸ‰\":\"ì–‘\", \"ë…€\":\"ì—¬\", \n",
        "          \"ë ¤\":\"ì—¬\", \"ë…\":\"ì—­\", \"ë ¥\":\"ì—­\", \"ë…„\":\"ì—°\", \"ë ¨\":\"ì—°\", \"ë…ˆ\":\"ì—´\", \"ë ¬\":\"ì—´\", \n",
        "          \"ë…\":\"ì—¼\", \"ë ´\":\"ì—¼\", \"ë µ\":\"ì—½\", \"ë…•\":\"ì˜\", \"ë ¹\":\"ì˜\", \"ë…œ\":\"ì˜ˆ\", \"ë¡€\":\"ì˜ˆ\", \n",
        "          \"ë¡œ\":\"ë…¸\", \"ë¡\":\"ë…¹\", \"ë¡ \":\"ë…¼\", \"ë¡±\":\"ë†\", \"ë¢°\":\"ë‡Œ\", \"ë‡¨\":\"ìš”\", \"ë£Œ\":\"ìš”\", \n",
        "          \"ë£¡\":\"ìš©\", \"ë£¨\":\"ëˆ„\", \"ë‰´\":\"ìœ \", \"ë¥˜\":\"ìœ \", \"ë‰µ\":\"ìœ¡\", \"ë¥™\":\"ìœ¡\", \"ë¥œ\":\"ìœ¤\", \n",
        "          \"ë¥ \":\"ìœ¨\", \"ë¥­\":\"ìœµ\", \"ë¥µ\":\"ëŠ‘\", \"ë¦„\":\"ëŠ \", \"ë¦‰\":\"ëŠ¥\", \"ë‹ˆ\":\"ì´\", \"ë¦¬\":\"ì´\", \n",
        "          \"ë¦°\":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}\n",
        "    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list\n",
        "    res_l = []\n",
        "\n",
        "    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # ë‘ìŒ ë²•ì¹™ ì ìš©\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì\n",
        "        if idx == 0:\n",
        "            # ì²« ê¸€ì ì¸ì½”ë”©\n",
        "            input_ids = tokenizer.encode(\n",
        "            val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            \n",
        "            # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=5, temperature=0.9, repetition_penalty=1.5)\n",
        "        \n",
        "        # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´\n",
        "        else:\n",
        "            # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ë¬¸ì¥ì´ë‘ í˜„ì¬ ìŒì ˆ ì—°ê²°\n",
        "            # ì´í›„ generate ëœ ë¬¸ì¥ì—ì„œ ì´ì „ ë¬¸ì¥ì— ëŒ€í•œ ë°ì´í„° ì œê±°\n",
        "            link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "            # print(link_with_pre_sentence)\n",
        "\n",
        "            # ì—°ê²°ëœ ë¬¸ì¥ì„ ì¸ì½”ë”©\n",
        "            input_ids = tokenizer.encode(\n",
        "            link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "            # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=len_sequence, temperature=0.9, repetition_penalty=1.5)\n",
        "\n",
        "        # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)\n",
        "        generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "        # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°\n",
        "        if tokenizer.pad_token_id in generated_sequence:\n",
        "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "        \n",
        "        # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°\n",
        "        # print(generated_sequence)\n",
        "        if idx != 0:\n",
        "            # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°\n",
        "            generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            # ë‹¤ìŒ ìŒì ˆì„ ìœ„í•´ ê¸¸ì´ ê°±ì‹ \n",
        "            len_sequence += len(generated_sequence)        \n",
        "        \n",
        "        # ì²« ê¸€ìë¼ë©´\n",
        "        else:\n",
        "            # ì‹œí€€ìŠ¤ ê¸¸ì´ ì €ì¥\n",
        "            len_sequence = len(generated_sequence)\n",
        "        \n",
        "        # print(last_sequence)\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë””ì½”ë”©\n",
        "        decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "    poem_dict = {}\n",
        "\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        poem_dict[poem(letter)] = res\n",
        "\n",
        "    return poem_dict\n",
        "\n",
        "###\n",
        "\n",
        "# Image(.gif)\n",
        "@st.cache(show_spinner=False)\n",
        "def load_lottieurl(url: str):\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "lottie_url = \"https://assets7.lottiefiles.com/private_files/lf30_fjln45y5.json\"\n",
        "\n",
        "lottie_json = load_lottieurl(lottie_url)\n",
        "st_lottie(lottie_json, speed=1, height=200, key=\"initial\")\n",
        "\n",
        "\n",
        "# Title\n",
        "row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.columns(\n",
        "    (0.01, 2, 0.05, 1, 0.01)\n",
        ")\n",
        "\n",
        "with row0_1:\n",
        "    st.title(\"í•œê¸€ ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œ\")\n",
        "    st.subheader(\"ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AIS7 íŒŒì´ë„ í”„ë¡œì íŠ¸\")\n",
        "\n",
        "with row0_2:\n",
        "    st.write(\"\")\n",
        "    st.subheader(\n",
        "        \"í•´íŒŒë¦¬íŒ€\"\n",
        "    )\n",
        "    st.write(\"ì´ì§€í˜œ, ìµœì§€ì˜, ê¶Œì†Œí¬\")\n",
        "    st.write(\"ë¬¸ì¢…í˜„, êµ¬ìí˜„, ê¹€ì˜ì¤€\")\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Explanation\n",
        "row1_spacer1, row1_1, row1_spacer2 = st.columns((0.01, 3, 0.01))\n",
        "\n",
        "with row1_1:\n",
        "    st.markdown(\n",
        "        \"**'MZì„¸ëŒ€'ì—ê²Œ**\"\n",
        "    )\n",
        "    st.markdown(\n",
        "        \"ìŒì•…ì€ ì„¸ëŒ€ë¥¼ ë“œëŸ¬ë‚´ëŠ” ì§€í‘œì´ì ìì‹ ì˜ ê°ì • ë° ê³µë™ì²´ë¥¼ ë“œëŸ¬ë‚´ëŠ” ìˆ˜ë‹¨ì´ë‹¤.\"\n",
        "    )\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Model & Input\n",
        "row2_spacer1, row2_1, row2_spacer2 = st.columns((0.01, 1.5, 0.05))\n",
        "\n",
        "# Word Input\n",
        "if \"generate\" not in st.session_state:\n",
        "    st.session_state.generate = False\n",
        "\n",
        "with row2_1:\n",
        "    word_input = st.text_input(\n",
        "            \"ní–‰ì‹œì— ì‚¬ìš©í•  ë‹¨ì–´ë¥¼ ì ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”. ğŸ‘‡\",\n",
        "            placeholder='í•œê¸€ ë‹¨ì–´',\n",
        "            max_chars=10\n",
        "    )\n",
        "    \n",
        "    if word_input:\n",
        "        st.write(\"ní–‰ì‹œ ë‹¨ì–´ :  \", word_input)\n",
        "\n",
        "    if st.button('ní–‰ì‹œ ì œì‘í•˜ê¸°'):\n",
        "        with st.spinner('ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):\n",
        "            result = n_line_poem(word_input)\n",
        "        st.success('ì™„ë£ŒëìŠµë‹ˆë‹¤!')\n",
        "        for r in result:\n",
        "            st.write(f'{r} : {result[r]}')"
      ],
      "metadata": {
        "id": "msfSlTM9kyXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor, cat, reshape\n",
        "t1 = tensor([[ 9208,  6866, 12422, 31656,  7285,  9065]])\n",
        "t2 = tensor([[ 9208,  6866, 12422, 31656,  7285,  9111]])\n",
        "\n",
        "t = cat((t1[0], t2[0]), 0)\n",
        "print(t)\n",
        "print(len(t))\n",
        "reshape(t, (1, 12))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WdxvVZzeH3U",
        "outputId": "f32029bd-b4a3-4e51-87de-7c11afc8eb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 9208,  6866, 12422, 31656,  7285,  9065,  9208,  6866, 12422, 31656,\n",
            "         7285,  9111])\n",
            "12\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9208,  6866, 12422, 31656,  7285,  9065,  9208,  6866, 12422, 31656,\n",
              "          7285,  9111]])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    }
  ]
}