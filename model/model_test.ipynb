{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "0EGVSgLnMDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG8EnQP5dKH4",
        "outputId": "b7a75837-23ce-40fb-94c0-1de3b960b0bb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ëª¨ë¸ í˜¸ì¶œ"
      ],
      "metadata": {
        "id": "ktZz2tUjMQE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_project_kogpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_project_kogpt2\")"
      ],
      "metadata": {
        "id": "cu-VSwoVL7hK"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ì‚¼í–‰ì‹œ í•¨ìˆ˜"
      ],
      "metadata": {
        "id": "N8ZU3pQpMrz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPU ì‚¬ìš© ì—¬ë¶€\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4esBql1YQNB",
        "outputId": "b76933ba-2e33-4139-bb50-e9428acedc80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mind(input_letter):\n",
        "    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „\n",
        "    dooeum = {\"ë¼\":\"ë‚˜\", \"ë½\":\"ë‚™\", \"ë€\":\"ë‚œ\", \"ë„\":\"ë‚ \", \"ëŒ\":\"ë‚¨\", \"ë\":\"ë‚©\", \"ë‘\":\"ë‚­\", \n",
        "          \"ë˜\":\"ë‚´\", \"ë­\":\"ëƒ‰\", \"ëƒ‘\":\"ì•½\", \"ëµ\":\"ì•½\", \"ëƒ¥\":\"ì–‘\", \"ëŸ‰\":\"ì–‘\", \"ë…€\":\"ì—¬\", \n",
        "          \"ë ¤\":\"ì—¬\", \"ë…\":\"ì—­\", \"ë ¥\":\"ì—­\", \"ë…„\":\"ì—°\", \"ë ¨\":\"ì—°\", \"ë…ˆ\":\"ì—´\", \"ë ¬\":\"ì—´\", \n",
        "          \"ë…\":\"ì—¼\", \"ë ´\":\"ì—¼\", \"ë µ\":\"ì—½\", \"ë…•\":\"ì˜\", \"ë ¹\":\"ì˜\", \"ë…œ\":\"ì˜ˆ\", \"ë¡€\":\"ì˜ˆ\", \n",
        "          \"ë¡œ\":\"ë…¸\", \"ë¡\":\"ë…¹\", \"ë¡ \":\"ë…¼\", \"ë¡±\":\"ë†\", \"ë¢°\":\"ë‡Œ\", \"ë‡¨\":\"ìš”\", \"ë£Œ\":\"ìš”\", \n",
        "          \"ë£¡\":\"ìš©\", \"ë£¨\":\"ëˆ„\", \"ë‰´\":\"ìœ \", \"ë¥˜\":\"ìœ \", \"ë‰µ\":\"ìœ¡\", \"ë¥™\":\"ìœ¡\", \"ë¥œ\":\"ìœ¤\", \n",
        "          \"ë¥ \":\"ìœ¨\", \"ë¥­\":\"ìœµ\", \"ë¥µ\":\"ëŠ‘\", \"ë¦„\":\"ëŠ \", \"ë¦‰\":\"ëŠ¥\", \"ë‹ˆ\":\"ì´\", \"ë¦¬\":\"ì´\", \n",
        "          \"ë¦°\":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}\n",
        "    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list\n",
        "    res_l = []\n",
        "\n",
        "    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # ë‘ìŒ ë²•ì¹™ ì ìš©\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        times = 0\n",
        "        while times < 3:\n",
        "            # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì\n",
        "            if idx == 0:\n",
        "                # ì²« ê¸€ì ì¸ì½”ë”©\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                \n",
        "                # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "            \n",
        "            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´\n",
        "            else:\n",
        "                # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ë¬¸ì¥ì´ë‘ í˜„ì¬ ìŒì ˆ ì—°ê²°\n",
        "                # ì´í›„ generate ëœ ë¬¸ì¥ì—ì„œ ì´ì „ ë¬¸ì¥ì— ëŒ€í•œ ë°ì´í„° ì œê±°\n",
        "                link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "                # print(link_with_pre_sentence)\n",
        "\n",
        "                # ì—°ê²°ëœ ë¬¸ì¥ì„ ì¸ì½”ë”©\n",
        "                input_ids = tokenizer.encode(\n",
        "                link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "                # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=len_sequence + 5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)\n",
        "\n",
        "            # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)\n",
        "            generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "            # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "                # ë‹¤ìŒ ìŒì ˆì„ ìœ„í•´ ê¸¸ì´ ê°±ì‹ \n",
        "                len_sequence += len(generated_sequence)        \n",
        "            \n",
        "            # ì²« ê¸€ìë¼ë©´\n",
        "            else:\n",
        "                # ì‹œí€€ìŠ¤ ê¸¸ì´ ì €ì¥\n",
        "                len_sequence = len(generated_sequence)\n",
        "\n",
        "            # ê²°ê³¼ë¬¼ ë””ì½”ë”©\n",
        "            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "            if len(decoded_sequence) > 1:\n",
        "                break\n",
        "            else:\n",
        "                times += 1\n",
        "                continue\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "        # print(res_l)\n",
        "\n",
        "    # ê²°ê³¼ë¬¼ listì—ì„œ í•œ ì¤„ì”© ì¶œë ¥\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        print(f\"{letter} :\", res)"
      ],
      "metadata": {
        "id": "zZgiBQRtYDqc"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë°°ê³ íŒŒ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPh0aX8gtnXt",
        "outputId": "c2773fdd-ae90-4949-d96c-f0ba181fa983"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°° : ë°°íŠ¸ë§¨ê³¼ ì¡°ì»¤\n",
            "ê³  : ê³ ì € ì¼€ì¸ë“¤\n",
            "íŒŒ : íŒŒë‘ ë˜ë“¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì½”ì½”ì•„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz_CjqVUtqCM",
        "outputId": "4ac151b7-2ada-4552-ed83-8012de124447"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì½” : ì½”ëì´ ì°¡í•œ ê²ƒì„ ëŠë¼ì§€\n",
            "ì½” : ì½” ëì´ ë‹¿ê²Œ\n",
            "ì•„ : ì•„ë ¨í•œ ê¸°ì–µ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"í•´íŒŒë¦¬\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SRF781-kFoc",
        "outputId": "9597a1a0-acb8-48d3-fabb-e69ac14f9ead"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í•´ : í•´ë§‘ê²Œ ì›ƒì–´ì£¼ëŠ” ë„ˆ\n",
            "íŒŒ : íŒŒë…¸ì„ì²˜ëŸ¼\n",
            "ë¦¬ : ì´ ê¸¸ì„ ë‹¬ë ¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì•„ì´ìŠ¤í¬ë¦¼\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdHiRak_iKk5",
        "outputId": "246341af-d7a9-4a82-9f2a-74c52d4c413e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•„ : ì•„ë ¨í•˜ê²Œ ì†ì‚­ì´ëŠ”\n",
            "ì´ : ì´ í•œë§ˆë””\n",
            "ìŠ¤ : ìŠ¤ì³ê°€ëŠ” ì¸ì—°ì´ê¸¸\n",
            "í¬ : í¬ê² ì–´ìš”\n",
            "ë¦¼ : ì„ì„ë†“ì§€ ì•ŠëŠ” ì•„ë¦„ë‹¤ìš´ ê·¸ëŒ€ ê·¸ë¦¬ìš´ ë§ë“¤ì€ ì‚¬ë¼ì ¸ ê°€ë„¤ ì—¬ê¸° ì„œìˆëŠ”ë° ì ˆëŒ€ ë’¤ëŒì•„ë³´ì§€ë§ˆ í•˜ì§€ë§Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"íŒŒì´ë„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH1WvJYUYGH8",
        "outputId": "6c5afaee-5e43-4b9c-e5d9-7a87be1066ba"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŒŒ : íŒŒë„ì— ì‚¬ë¼ì ¸\n",
            "ì´ : ì´ì  \n",
            "ë„ : ë„ ìŠì–´ë³´ë ¤ê³  í•´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë°•ê²½íƒ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO6RlCBG0hYE",
        "outputId": "beb5af6e-a111-4882-c461-44535990e81c"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°• : ë°•í˜€ìˆëŠ” ê·¸ ì¡°ê°ë“¤ì„\n",
            "ê²½ : ê²½ì í•´\n",
            "íƒ : íƒì‹œ ëë‚¬ì–´ ì²¨ìœ¼ë¡œ ëˆˆì„ ë—„ ìˆ˜ ì—†ì—ˆì§€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë¬¸ì¢…í˜„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaVSRcL2YIW0",
        "outputId": "bb9ff9d0-3af1-47df-ca7e-08b9a7a60f06"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ : ë¬¸ë“ ê·¸ë¦¬ìš´ ë§˜ì— ì „í™”ë¥¼ ê±¸ì£ \n",
            "ì¢… : ì¢…ì¼\n",
            "í˜„ : í˜„ì”í•œ ê¸°ë¶„ì˜¤ë˜\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì´ì •ì€\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9TsrSVZabb8",
        "outputId": "840b358c-127a-4c94-877a-24a77f1d8c66"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ : ì´ ë°¤ì´ ìƒˆë„ë¡ ë„ ê¸°ë‹¤ë ¤\n",
            "ì • : ì •ë§ì´ì•¼\n",
            "ì€ : ì€ë¹›ì€ ìš°ë¦´ ëŒë ¤ì£¼ë„¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ê¶Œì†Œí¬\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl9KQ8xRy_PD",
        "outputId": "52d10e88-a4ff-4836-9997-d9baaeaddfc3"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê¶Œ : ê¶Œíƒœê¸°ê°€ ì•„ëƒ ì´ê±´ ë ëŒ€ì²´ ì–´ë””ë¶€í„° ì˜ëª»ëœê±´ì§€\n",
            "ì†Œ : ì†Œìš©ì´ ì—†ì–´\n",
            "í¬ : í¬ë¯¸í•œ ë¶ˆë¹›\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ë°•ê±´ì˜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc2DtSbVcK9-",
        "outputId": "a5f2ac40-c6d9-4aa6-b10a-ae708eef91c3"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë°• : ë°•ìˆ˜ë¥¼ ì¹˜ì\n",
            "ê±´ : ê±´ë„¤ì£¼ë¦¬\n",
            "ì˜ : ì˜ì˜ ì°¨ê°€ìš´ ì‹œì„ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì •ì¬ì˜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PItWGW0xcQFN",
        "outputId": "983a9e1e-2a98-4454-e769-471f51d44aa6"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì • : ì •ì²˜ ì—†ì´ ê±·ë‹¤ê°€\n",
            "ì¬ : ì¬ì´‰í•˜ëŠ” ë‚¯ì„  ë°œê±¸ìŒì€\n",
            "ì˜ : ì˜ì›ì´ê²Œ ë©€ì–´ì§€ë„¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"êµ¬ìí˜„\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCE22o8bzhbM",
        "outputId": "8473bd79-63cd-4cf3-b4bf-e8bc2d0d7cc4"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "êµ¬ : êµ¬ê²¨ì ¸ì§„ ì§€ë‚œ ê¸°ì–µ\n",
            "ì : ìê¾¸ë§Œ\n",
            "í˜„ : í˜„ì•„\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ê¹€ì˜ì¤€\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfiH0W_K1YUQ",
        "outputId": "f26abb38-0b7a-4998-85c8-1a3c313e1a63"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ê¹€ : ê¹€ê±´ëª¨ ë…¸ë˜\n",
            "ì˜ : ì˜ìì™€\n",
            "ì¤€ : ì¤€ë‹´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ìµœì§€ì˜\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuDuEWtscqwG",
        "outputId": "94613f2b-c818-484c-9795-055332646c80"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìµœ : ìµœí¬ ì…ë‹ˆë‹¤\n",
            "ì§€ : ì§€ì¹œ ë§˜ì„\n",
            "ì˜ : ì˜ì˜ ë‹«í˜€ì§„ ë¬¸ í‹ˆì—ì„œ ë‚œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mind(\"ì´ì§€í˜œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I69iP1ombVg",
        "outputId": "440b60f2-78b7-4763-ef30-e120f0a3433c"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ : ì´ì œì™€ì„œ ë¹„ì›Œì•¼í•˜ë‹ˆ ìì‹ ì´ ì—†ì–´\n",
            "ì§€ : ì§€ìš°ë ¤ í•´ë„ ë§Œì§ˆ ìˆ˜ ì—†ëŠ” ê±¸\n",
            "í˜œ : í˜œì˜¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í˜¹ì‹œ ëª°ë¼ ì €ì¥\n",
        "* ì™€ì¼ë¬¸ ì¶”ê°€ ì „ ìŠ¤íŠ¸ë¦¼ë¦¿"
      ],
      "metadata": {
        "id": "7gfY8cxwBPsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import streamlit as st\n",
        "from streamlit_lottie import st_lottie\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Page Config\n",
        "st.set_page_config(\n",
        "    page_title=\"ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œ\",\n",
        "    page_icon=\"ğŸ’Œ\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "### Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_20man\")\n",
        "\n",
        "@st.cache(show_spinner=False)\n",
        "def load_model():\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_20man\")\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Class : Dict ì¤‘ë³µ í‚¤ ì¶œë ¥\n",
        "class poem(object):\n",
        "    def __init__(self,letter):\n",
        "        self.letter = letter\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.letter\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"'\"+self.letter+\"'\"\n",
        "\n",
        "\n",
        "def n_line_poem(input_letter):\n",
        "\n",
        "    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „\n",
        "    dooeum = {\"ë¼\":\"ë‚˜\", \"ë½\":\"ë‚™\", \"ë€\":\"ë‚œ\", \"ë„\":\"ë‚ \", \"ëŒ\":\"ë‚¨\", \"ë\":\"ë‚©\", \"ë‘\":\"ë‚­\", \n",
        "          \"ë˜\":\"ë‚´\", \"ë­\":\"ëƒ‰\", \"ëƒ‘\":\"ì•½\", \"ëµ\":\"ì•½\", \"ëƒ¥\":\"ì–‘\", \"ëŸ‰\":\"ì–‘\", \"ë…€\":\"ì—¬\", \n",
        "          \"ë ¤\":\"ì—¬\", \"ë…\":\"ì—­\", \"ë ¥\":\"ì—­\", \"ë…„\":\"ì—°\", \"ë ¨\":\"ì—°\", \"ë…ˆ\":\"ì—´\", \"ë ¬\":\"ì—´\", \n",
        "          \"ë…\":\"ì—¼\", \"ë ´\":\"ì—¼\", \"ë µ\":\"ì—½\", \"ë…•\":\"ì˜\", \"ë ¹\":\"ì˜\", \"ë…œ\":\"ì˜ˆ\", \"ë¡€\":\"ì˜ˆ\", \n",
        "          \"ë¡œ\":\"ë…¸\", \"ë¡\":\"ë…¹\", \"ë¡ \":\"ë…¼\", \"ë¡±\":\"ë†\", \"ë¢°\":\"ë‡Œ\", \"ë‡¨\":\"ìš”\", \"ë£Œ\":\"ìš”\", \n",
        "          \"ë£¡\":\"ìš©\", \"ë£¨\":\"ëˆ„\", \"ë‰´\":\"ìœ \", \"ë¥˜\":\"ìœ \", \"ë‰µ\":\"ìœ¡\", \"ë¥™\":\"ìœ¡\", \"ë¥œ\":\"ìœ¤\", \n",
        "          \"ë¥ \":\"ìœ¨\", \"ë¥­\":\"ìœµ\", \"ë¥µ\":\"ëŠ‘\", \"ë¦„\":\"ëŠ \", \"ë¦‰\":\"ëŠ¥\", \"ë‹ˆ\":\"ì´\", \"ë¦¬\":\"ì´\", \n",
        "          \"ë¦°\":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}\n",
        "    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list\n",
        "    res_l = []\n",
        "\n",
        "    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # ë‘ìŒ ë²•ì¹™ ì ìš©\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì\n",
        "        if idx == 0:\n",
        "            # ì²« ê¸€ì ì¸ì½”ë”©\n",
        "            input_ids = tokenizer.encode(\n",
        "            val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "            \n",
        "            # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=5, temperature=0.9, repetition_penalty=1.5)\n",
        "        \n",
        "        # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´\n",
        "        else:\n",
        "            # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ë¬¸ì¥ì´ë‘ í˜„ì¬ ìŒì ˆ ì—°ê²°\n",
        "            # ì´í›„ generate ëœ ë¬¸ì¥ì—ì„œ ì´ì „ ë¬¸ì¥ì— ëŒ€í•œ ë°ì´í„° ì œê±°\n",
        "            link_with_pre_sentence = \" \".join(res_l) + \" \" + val  \n",
        "            # print(link_with_pre_sentence)\n",
        "\n",
        "            # ì—°ê²°ëœ ë¬¸ì¥ì„ ì¸ì½”ë”©\n",
        "            input_ids = tokenizer.encode(\n",
        "            link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "            # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±\n",
        "            output_sequence = model.generate(\n",
        "                input_ids=input_ids, \n",
        "                do_sample=True, max_length=42, no_repeat_ngram_size=2,\n",
        "                min_length=len_sequence, temperature=0.9, repetition_penalty=1.5)\n",
        "\n",
        "        # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)\n",
        "        generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "        # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°\n",
        "        if tokenizer.pad_token_id in generated_sequence:\n",
        "            generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "        \n",
        "        # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°\n",
        "        # print(generated_sequence)\n",
        "        if idx != 0:\n",
        "            # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°\n",
        "            generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            # ë‹¤ìŒ ìŒì ˆì„ ìœ„í•´ ê¸¸ì´ ê°±ì‹ \n",
        "            len_sequence += len(generated_sequence)        \n",
        "        \n",
        "        # ì²« ê¸€ìë¼ë©´\n",
        "        else:\n",
        "            # ì‹œí€€ìŠ¤ ê¸¸ì´ ì €ì¥\n",
        "            len_sequence = len(generated_sequence)\n",
        "        \n",
        "        # print(last_sequence)\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë””ì½”ë”©\n",
        "        decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "    poem_dict = {}\n",
        "\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        poem_dict[poem(letter)] = res\n",
        "\n",
        "    return poem_dict\n",
        "\n",
        "###\n",
        "\n",
        "# Image(.gif)\n",
        "@st.cache(show_spinner=False)\n",
        "def load_lottieurl(url: str):\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "lottie_url = \"https://assets7.lottiefiles.com/private_files/lf30_fjln45y5.json\"\n",
        "\n",
        "lottie_json = load_lottieurl(lottie_url)\n",
        "st_lottie(lottie_json, speed=1, height=200, key=\"initial\")\n",
        "\n",
        "\n",
        "# Title\n",
        "row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.columns(\n",
        "    (0.01, 2, 0.05, 1, 0.01)\n",
        ")\n",
        "\n",
        "with row0_1:\n",
        "    st.title(\"í•œê¸€ ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œ\")\n",
        "    st.subheader(\"ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AIS7 íŒŒì´ë„ í”„ë¡œì íŠ¸\")\n",
        "\n",
        "with row0_2:\n",
        "    st.write(\"\")\n",
        "    st.subheader(\n",
        "        \"í•´íŒŒë¦¬íŒ€\"\n",
        "    )\n",
        "    st.write(\"ì´ì§€í˜œ, ìµœì§€ì˜, ê¶Œì†Œí¬\")\n",
        "    st.write(\"ë¬¸ì¢…í˜„, êµ¬ìí˜„, ê¹€ì˜ì¤€\")\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Explanation\n",
        "row1_spacer1, row1_1, row1_spacer2 = st.columns((0.01, 3, 0.01))\n",
        "\n",
        "with row1_1:\n",
        "    st.markdown(\n",
        "        \"**'MZì„¸ëŒ€'ì—ê²Œ**\"\n",
        "    )\n",
        "    st.markdown(\n",
        "        \"ìŒì•…ì€ ì„¸ëŒ€ë¥¼ ë“œëŸ¬ë‚´ëŠ” ì§€í‘œì´ì ìì‹ ì˜ ê°ì • ë° ê³µë™ì²´ë¥¼ ë“œëŸ¬ë‚´ëŠ” ìˆ˜ë‹¨ì´ë‹¤.\"\n",
        "    )\n",
        "\n",
        "st.write('---')\n",
        "\n",
        "# Model & Input\n",
        "row2_spacer1, row2_1, row2_spacer2 = st.columns((0.01, 1.5, 0.05))\n",
        "\n",
        "# Word Input\n",
        "if \"generate\" not in st.session_state:\n",
        "    st.session_state.generate = False\n",
        "\n",
        "with row2_1:\n",
        "    word_input = st.text_input(\n",
        "            \"ní–‰ì‹œì— ì‚¬ìš©í•  ë‹¨ì–´ë¥¼ ì ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”. ğŸ‘‡\",\n",
        "            placeholder='í•œê¸€ ë‹¨ì–´',\n",
        "            max_chars=10\n",
        "    )\n",
        "    \n",
        "    if word_input:\n",
        "        st.write(\"ní–‰ì‹œ ë‹¨ì–´ :  \", word_input)\n",
        "\n",
        "    if st.button('ní–‰ì‹œ ì œì‘í•˜ê¸°'):\n",
        "        with st.spinner('ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):\n",
        "            result = n_line_poem(word_input)\n",
        "        st.success('ì™„ë£ŒëìŠµë‹ˆë‹¤!')\n",
        "        for r in result:\n",
        "            st.write(f'{r} : {result[r]}')"
      ],
      "metadata": {
        "id": "msfSlTM9kyXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}