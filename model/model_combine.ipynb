{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Uvy5CnQNU2nU",
        "85lk0J1OU2Mi",
        "mHK9b3GuP_zC",
        "_lnhmqn-gJ6i",
        "5ZBXRuQfZFIp",
        "Qt7rO4A9YVy7",
        "HE-ALuY0HBZb",
        "ArGDdO2pRJBg",
        "HQuqCy1TRjVA",
        "4u0VvaFRSFGE",
        "V4XhlubS0Z5r",
        "cswGVrK0vlbJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "SXLvklx9P6fK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리 설치"
      ],
      "metadata": {
        "id": "Uvy5CnQNU2nU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTyVsK8iYmXC",
        "outputId": "9e2a8bd9-eded-48ca-8647-2a2c9186ea8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 98.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 87.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KoGPT2 model, tokenizer 호출"
      ],
      "metadata": {
        "id": "85lk0J1OU2Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "                                                    pad_token='<pad>', mask_token='<mask>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186,
          "referenced_widgets": [
            "e7c74d42819743ba99aae13df908e8cf",
            "a9a048ed483c486cb2315cc0c6819b0b",
            "78bb11e8a7504ac487f6fac0eb1595a8",
            "fec20a7173894742a0820df8b9220244",
            "e7036541d8a747049a94e8bc72602f90",
            "9bc66764ddb244ccb747dc272c67d318",
            "cddbbe3468b643648d444946849da4d4",
            "207000fb1aca4e4ba1ba279c58f19b22",
            "135634df2e554b9f9ca5cb29333bdabb",
            "86c543375ec846fdac56a508a94ad491",
            "401b0e0d55204dec82b2b86544e55847",
            "db9599aa4bdf490ebd98fb68f1d6d800",
            "7d2b8803293c471b9705d4f9f508355d",
            "a29a59b44db9484e9dcacfadb96a54e2",
            "934381a6062c42cd8686577cf5e9d6ac",
            "56458ef30ccb4085b9022fa1c345a0e3",
            "9c2e05aa86f446aaa6c17f5a571bfe11",
            "a8a7f0cb3aeb4337a186725e80469189",
            "eea0f2a43c3443e382a3222aff063b10",
            "a3f058428e9b46759d4730e6a6a7c4b0",
            "4eb761028e5e417aad033d3d38ffd62f",
            "9436653feb764514bf5357309ddd8bd5",
            "999107066b954c348f44535de7177363",
            "63c6902b16024c81adcce17a86e34863",
            "34f67e212f4948be8f6b2a86d5e0af93",
            "91dd4b67d9a3483aa108e24bc46882e5",
            "c7f1112bb82143038f2c2ca402d1a176",
            "c99c728784534281911ea64944d51bfa",
            "a44ae11d49d34b679eaab8665a68dc5b",
            "662722bd98b4493785a16c732712ef65",
            "a2930f48666f43219b18a4095d941f1c",
            "ff211131e9b9458988eee7d68bd99fd9",
            "b4527fa9878a49c1880732eb2e378325"
          ]
        },
        "id": "_M6dgrMXY12t",
        "outputId": "7eb95c79-489b-4aa9-dba6-639b4cc9476c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7c74d42819743ba99aae13df908e8cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db9599aa4bdf490ebd98fb68f1d6d800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "999107066b954c348f44535de7177363"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU 사용 여부"
      ],
      "metadata": {
        "id": "mHK9b3GuP_zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL1xlCeJaXQD",
        "outputId": "496dda3a-f91f-42e0-e5b3-3542ecc76154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파인 튜닝 데이터"
      ],
      "metadata": {
        "id": "_lnhmqn-gJ6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_mGDLrqEZBpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 발라드 약 56,000곡의 가사 데이터\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/data/ballad_all.txt\", 'r')\n",
        "lines = f.readlines()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "RIGjAI94ZKYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 개수, 약 96만개\n",
        "print(len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLossn0NhX5Z",
        "outputId": "edef08dd-b6f8-4a77-891e-866c0511ce24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "961858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tokenizer"
      ],
      "metadata": {
        "id": "5ZBXRuQfZFIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 문장 500,000개만 샘플링해서 일단 돌려보자\n",
        "# man_lines = list(np.random.choice(lines, 500000))\n",
        "# removed_lines = [line.replace('\\n', '') for line in man_lines]\n",
        "# list(np.random.choice(removed_lines, 10))"
      ],
      "metadata": {
        "id": "OfKHKgF_hfSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_lines = [line.replace('\\n', '') for line in lines[600000:]]\n",
        "removed_lines[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG2P-TMiZRtf",
        "outputId": "6862271a-e9ff-4295-e134-00b38988219d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['수줍게 내게 다가와',\n",
              " '외로운 새벽에 익숙해질까',\n",
              " '작은 니 손을 잡고 있는 나',\n",
              " '1분 1초 매 순간 기억해',\n",
              " '눈물 마저 지쳐 그댈 잊기전에']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenizer(removed_lines, \n",
        "                               return_tensors=\"pt\", \n",
        "                               padding=\"max_length\", \n",
        "                               max_length=42,\n",
        "                               truncation=True)"
      ],
      "metadata": {
        "id": "YMIgBrMvZRre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUkYNYLKZRpC",
        "outputId": "39fabe6d-c480-40cb-f242-06e647157097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 9025,  8241,  6866,  ...,     3,     3,     3],\n",
              "        [ 9256,  9520, 46107,  ...,     3,     3,     3],\n",
              "        [ 9836, 11523, 15309,  ...,     3,     3,     3],\n",
              "        ...,\n",
              "        [39194, 17077,  6889,  ...,     3,     3,     3],\n",
              "        [10351, 25867, 12487,  ...,     3,     3,     3],\n",
              "        [12817,  8168, 13278,  ...,     3,     3,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/transformers/custom_datasets.html\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\" CustomDataset class for poetic sentences \"\"\"\n",
        "    def __init__(self, list_dataset, tokenizer):\n",
        "\n",
        "        self.list_dataset = list_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tokenized_sentences = self.tokenizer(\n",
        "            list_dataset,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=42,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=False,\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded_dict = {key: val[idx] for key, val in self.tokenized_sentences.items()}\n",
        "        encoded_dict[\"labels\"] = encoded_dict[\"input_ids\"].clone() # gpt has same labels as input_ids: https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
        "        return encoded_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_dataset)"
      ],
      "metadata": {
        "id": "JHJe_wItemMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = CustomDataset(removed_lines, tokenizer)"
      ],
      "metadata": {
        "id": "HYDgtbrpepth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 파인 튜닝"
      ],
      "metadata": {
        "id": "Qt7rO4A9YVy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Trainer가 학습, 평가에 사용할 모든 하이퍼 파라미터를 포함하는 클래스 정의\n",
        "# 학습된 모델이 저장될 디렉토리만 지정하고 나머지는 기본값 사용\n",
        "training_args = TrainingArguments(\n",
        "    # 모델 저장 경로\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/ballad_all_model\",\n",
        "    # 에포크 수\n",
        "    num_train_epochs=10,\n",
        "    # 메모리 절약\n",
        "    gradient_checkpointing=False,\n",
        "    # 매번 로그 찍을 스텝\n",
        "    logging_steps=10000,\n",
        "    warmup_steps=100000,\n",
        "    save_steps=100000,\n",
        "    eval_steps=100000)"
      ],
      "metadata": {
        "id": "pNW34ApQZRXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda8e8ca-0d61-42d5-fa3a-e1625df244c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    eval_dataset=tokenized_datasets,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "Gc_VQj5NYoL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1KvDFS6dZfkm",
        "outputId": "499ad35f-491b-4983-9def-5b06ae57e313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 361858\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 452330\n",
            "  Number of trainable parameters = 125164032\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='452330' max='452330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [452330/452330 7:28:48, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.576500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.574900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.558400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>0.556600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80000</td>\n",
              "      <td>0.559500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90000</td>\n",
              "      <td>0.567300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100000</td>\n",
              "      <td>0.523500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110000</td>\n",
              "      <td>0.536400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120000</td>\n",
              "      <td>0.539500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130000</td>\n",
              "      <td>0.543200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140000</td>\n",
              "      <td>0.515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150000</td>\n",
              "      <td>0.484000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160000</td>\n",
              "      <td>0.494300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170000</td>\n",
              "      <td>0.497800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180000</td>\n",
              "      <td>0.503700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190000</td>\n",
              "      <td>0.439100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200000</td>\n",
              "      <td>0.444400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210000</td>\n",
              "      <td>0.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220000</td>\n",
              "      <td>0.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230000</td>\n",
              "      <td>0.433600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240000</td>\n",
              "      <td>0.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250000</td>\n",
              "      <td>0.406500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260000</td>\n",
              "      <td>0.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270000</td>\n",
              "      <td>0.416900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280000</td>\n",
              "      <td>0.362200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290000</td>\n",
              "      <td>0.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300000</td>\n",
              "      <td>0.370500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310000</td>\n",
              "      <td>0.375200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320000</td>\n",
              "      <td>0.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330000</td>\n",
              "      <td>0.326400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340000</td>\n",
              "      <td>0.333600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350000</td>\n",
              "      <td>0.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360000</td>\n",
              "      <td>0.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370000</td>\n",
              "      <td>0.305400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380000</td>\n",
              "      <td>0.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390000</td>\n",
              "      <td>0.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400000</td>\n",
              "      <td>0.308100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410000</td>\n",
              "      <td>0.299600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420000</td>\n",
              "      <td>0.279400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430000</td>\n",
              "      <td>0.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440000</td>\n",
              "      <td>0.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450000</td>\n",
              "      <td>0.283100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-100000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-100000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-100000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-100000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-100000/special_tokens_map.json\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-200000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-200000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-200000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-200000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-200000/special_tokens_map.json\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-300000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-300000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-300000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-300000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-300000/special_tokens_map.json\n",
            "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-400000\n",
            "Configuration saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-400000/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-400000/pytorch_model.bin\n",
            "tokenizer config file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-400000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/Colab Notebooks/ballad_all_model/checkpoint-400000/special_tokens_map.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=452330, training_loss=0.43137149279062714, metrics={'train_runtime': 26928.626, 'train_samples_per_second': 134.377, 'train_steps_per_second': 16.797, 'total_flos': 7.756104900096e+16, 'train_loss': 0.43137149279062714, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 저장"
      ],
      "metadata": {
        "id": "HE-ALuY0HBZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"wumusill/final_project_kogpt2\"\n",
        "token = \"huggingface_token\""
      ],
      "metadata": {
        "id": "Nf7OpvjEeBu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Upload to Huggingface Hub\n",
        "model.push_to_hub(\n",
        "    repo_name, \n",
        "    use_temp_dir=True, \n",
        "    use_auth_token=token\n",
        ")\n",
        "tokenizer.push_to_hub(\n",
        "    repo_name, \n",
        "    use_temp_dir=True, \n",
        "    use_auth_token=token\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWlUrMFxeC3C",
        "outputId": "e7e19f5b-14f1-4186-a403-f497dea0f017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /tmp/tmpxy6nzxxw/config.json\n",
            "Model weights saved in /tmp/tmpxy6nzxxw/pytorch_model.bin\n",
            "Uploading the following files to wumusill/final_project_kogpt2: config.json,pytorch_model.bin\n",
            "tokenizer config file saved in /tmp/tmpq6dwlga6/tokenizer_config.json\n",
            "Special tokens file saved in /tmp/tmpq6dwlga6/special_tokens_map.json\n",
            "Uploading the following files to wumusill/final_project_kogpt2: special_tokens_map.json,tokenizer.json,tokenizer_config.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/wumusill/final_project_kogpt2/commit/3b601d060d227925046f35a500df23a6370cfad3', commit_message='Upload tokenizer', commit_description='', oid='3b601d060d227925046f35a500df23a6370cfad3', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 호출"
      ],
      "metadata": {
        "id": "ArGDdO2pRJBg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "95e0e09dd00e4c90b0667694e9265ae3",
            "2c5a87cb971f4708b44de6d091b6c853",
            "40efed17b2ce482694d56ae475ff2730",
            "87f45194465a4bf292bdd21755dbe746",
            "b3178c4f0a614efb9297f3ab4b6a6174",
            "f15b92229fdf4e16b60aaf9ef2e4187b",
            "d60fd53d1a66461db4aab685b5cc9e22",
            "160a1d381aea4d3ca79d284ef07acc63",
            "7eb094d642fe4ffda8f98be1b784a222",
            "a904881962d144b9bbcc7545b5115d34",
            "85d9f3ae004e41fd908fd684ce453937",
            "49950d34613f4eb2bf212c80cd8d70c3",
            "d207333830cb4ec2a50b0a31cf5426df",
            "c10935c320054e90abeefaaf94ca817e",
            "0ac9af3cfc4d4487b98e2ae817655e33",
            "e56df14256604d29973f722d435fcc38",
            "5e748b47334b4ea5baedf7cd7094541b",
            "20f46c20c0f24cc0965b70b2b89820e7",
            "b84c13b55fc84e978191e47bbf7b4dfd",
            "a33c32d179054be79999bbb50710f3b9",
            "9fc5b46d7f1a42b587a217769b93a9eb",
            "00e339bfb07548d4af0c26ef7fe96ba3",
            "77a4c92022ed4be8b9fa2bb8bc17051d",
            "0c2562338d9d414b94e3213aad136b76",
            "6281066321db4dba8ae5618778976fa5",
            "23790e168e644a94a89ac393ae117a2c",
            "99efc36218634f75a1ac637022f2c5cb",
            "1dc3528ca616434a9b771fe14579ba7b",
            "fbe7c9d4f8434457bda3d3909225ec94",
            "1bc5d38d4de344b99eda97f82d9f62f7",
            "49a229dc4edd4fb59236dd6418e49845",
            "4604e751ce354750bab85e6f046a19da",
            "89f517662df24d42b32c21a7eddf1fda",
            "c3b96d1856954798b3e0124c52c8246b",
            "296ee4cf12b3404f969fc7234bcb510a",
            "615027d55ad54c98a37e3df358dd3a8a",
            "9c8cd28a53e14edabce0c433919c84a0",
            "0b34f3959f564f0abadc56d103782f9a",
            "d6625d4d93514574a4f04865076bb3ed",
            "5e1ebb1501704bd8bf983d61cc7de634",
            "ff6cecd19ed444dfbb968006b1f627fc",
            "f126132afa7f471c937815f0de4a41b9",
            "784a3b5dce8c482d9aa5984b978e6abe",
            "9633e7f1adf149b68fb821eb0bcdfe73",
            "9534f1566cda480483e7a6d2abd5a7ef",
            "60f37c44ad0a4282a8a6883c53114323",
            "23b55f89a4134837acb5505fb972f4cd",
            "026d8c3090424844908735b28e606920",
            "4f7aafe72aa64367a54c733a83b26aaa",
            "12aa3672693249b9a0d6c64b75cba639",
            "006d3c1680514460accabea109ac6ebe",
            "d11fb20f24004b0cb30d8a19e307dfaf",
            "6027143586064ac49a9836a3ca1db9cd",
            "b6b1745887fa4315b2adace704afad3e",
            "9951e0cea9a14691a4595f037e0426c8"
          ]
        },
        "id": "m8eAjFZgteJk",
        "outputId": "07271671-a950-43cd-fc1c-3f8b6c9c9c89"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95e0e09dd00e4c90b0667694e9265ae3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/301 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49950d34613f4eb2bf212c80cd8d70c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.25M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77a4c92022ed4be8b9fa2bb8bc17051d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3b96d1856954798b3e0124c52c8246b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9534f1566cda480483e7a6d2abd5a7ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/513M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"wumusill/final_project_kogpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"wumusill/final_project_kogpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 삼행시 함수 1"
      ],
      "metadata": {
        "id": "HQuqCy1TRjVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0OCoeB5teJo"
      },
      "outputs": [],
      "source": [
        "def long_line_poem(input_letter):\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "\n",
        "        while True:\n",
        "            # 만약 idx 가 0 이라면 == 첫 글자\n",
        "            if idx == 0:\n",
        "                # 첫 글자 인코딩\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                # print(f\"{idx}번 인코딩 : {input_ids}\\n\") # 2차원 텐서\n",
        "\n",
        "                # 첫 글자 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=input_ids.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)[0]\n",
        "                # print(\"첫 글자 인코딩 후 generate 결과:\", output_sequence, \"\\n\") # tensor\n",
        "\n",
        "            # 첫 글자가 아니라면\n",
        "            else:\n",
        "                # 한 음절\n",
        "                input_ids = tokenizer.encode(\n",
        "                val, add_special_tokens=False, return_tensors=\"pt\")\n",
        "                # print(f\"{idx}번 째 글자 인코딩 : {input_ids} \\n\")\n",
        "\n",
        "                # 좀더 매끄러운 삼행시를 위해 이전 인코딩과 지금 인코딩 연결\n",
        "                link_with_pre_sentence = torch.cat((generated_sequence, input_ids[0]), 0)\n",
        "                link_with_pre_sentence = torch.reshape(link_with_pre_sentence, (1, len(link_with_pre_sentence)))\n",
        "                # print(f\"이전 텐서와 연결된 텐서 {link_with_pre_sentence} \\n\")\n",
        "\n",
        "                # 인코딩 값으로 문장 생성\n",
        "                output_sequence = model.generate(\n",
        "                    input_ids=link_with_pre_sentence.to(device), \n",
        "                    do_sample=True, max_length=42,\n",
        "                    min_length=5, temperature=0.9, repetition_penalty=1.5,\n",
        "                    no_repeat_ngram_size=2)[0]\n",
        "                # print(f\"{idx}번 인코딩 후 generate : {output_sequence}\")\n",
        "        \n",
        "            # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "            generated_sequence = output_sequence.tolist()\n",
        "            # print(f\"{idx}번 인코딩 리스트 : {generated_sequence} \\n\")\n",
        "\n",
        "            # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "            if tokenizer.pad_token_id in generated_sequence:\n",
        "                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]\n",
        "            \n",
        "            generated_sequence = torch.tensor(generated_sequence) \n",
        "            # print(f\"{idx}번 인코딩 리스트 패딩 제거 후 다시 텐서 : {generated_sequence} \\n\")\n",
        "\n",
        "            # 첫 글자가 아니라면, generate 된 음절만 결과물 list에 들어갈 수 있게 앞 문장에 대한 인코딩 값 제거\n",
        "            # print(generated_sequence)\n",
        "            if idx != 0:\n",
        "                # 이전 문장의 길이 이후로 슬라이싱해서 앞 문장 제거\n",
        "                generated_sequence = generated_sequence[len_sequence:]\n",
        "\n",
        "            len_sequence = len(generated_sequence)\n",
        "            # print(\"len_seq\", len_sequence)\n",
        "\n",
        "            # 음절 그대로 뱉으면 다시 해와, 아니면 while문 탈출\n",
        "            if len_sequence > 1:\n",
        "                break\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(generated_sequence)\n",
        "\n",
        "        # print(\"res_l :\", res_l)\n",
        "\n",
        "    # 결과물 list에서 한 줄씩 출력\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        decode_res = tokenizer.decode(res, clean_up_tokenization_spaces=True)\n",
        "        print(f\"{letter} :\", decode_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 삼행시 함수 2"
      ],
      "metadata": {
        "id": "4u0VvaFRSFGE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4XhlubS0Z5r"
      },
      "source": [
        "### 발라드 단어 사전 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daLTY8ZzPufj",
        "outputId": "24a2cdc3-7e74-43e1-cc6a-685aa417a84d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(54743, 6)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root_path = \"../data/data_ballad\"\n",
        "\n",
        "# 학습 데이터\n",
        "all = pd.read_parquet(f\"{root_path}/ballad_all.gzip\")\n",
        "all.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm6JB2YgvlFJ"
      },
      "outputs": [],
      "source": [
        "# 가사에 개행문자 없는 데이터 제거\n",
        "all = all[all[\"가사\"].str.contains(\"\\n\")]\n",
        "# 그외 전처리\n",
        "all[\"가사\"] = all[\"가사\"].map(lambda x : re.sub(\"[^가-힣\\n ]\", \"\", x).strip()) # 한글 자음, 한글, 숫자, 개행문자만 남기고 제거\n",
        "all[\"가사\"] = all[\"가사\"].map(lambda x : re.sub(\"\\s{2,}\", \" \", x)) # 공백 2회 이상 제거\n",
        "all = all[all[\"가사\"].map(lambda x : len(x) > 10)] # 전처리 후 빈 행이나 10자 이상이 안되는 데이터 제거\n",
        "all = all.reset_index(drop=True) # 인덱스 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uHfTBeDvlMZ"
      },
      "outputs": [],
      "source": [
        "sentence_lst = []\n",
        "lyrics = all[\"가사\"].str.split(\"\\n\")\n",
        "for sentence in lyrics:\n",
        "    for word in sentence:\n",
        "        sentence_lst.extend(word.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-SOb_k7wTeo",
        "outputId": "e7e609bb-3e92-4702-b09e-428746a6f25c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6638809,)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = pd.Series(sentence_lst)\n",
        "sentence.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0osmrGY0wiNu",
        "outputId": "8c14d60d-c8e3-40d2-ca5b-a37b61648983"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(205504,)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = sentence.drop_duplicates()\n",
        "sentence.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F_82aslwiTg",
        "outputId": "569c6b98-d298-4cb9-a7d8-cb460db7044f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(204599,)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = sentence[sentence.str.len() > 1]\n",
        "sentence.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eGarFUgGwiZ3"
      },
      "outputs": [],
      "source": [
        "sentence = sentence.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xw6VT9Swie0",
        "outputId": "b7519176-700c-4ac0-e3bb-855a9d10c6e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       가가\n",
              "1    가가가만히\n",
              "2      가거든\n",
              "3     가거들랑\n",
              "4      가거라\n",
              "dtype: object"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = sentence.reset_index(drop=True)\n",
        "words.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hdjk8wqPufl"
      },
      "outputs": [],
      "source": [
        "words.to_csv(f\"{root_path}/ballad_ward.txt\", index=False, encoding=\"cp949\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39jRwhSyPufl",
        "outputId": "4e243cf5-5476-4590-a973-4becdecd481a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>가가</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>가가가만히</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>가거든</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>가거들랑</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>가거라</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204594</th>\n",
              "      <td>힙합은</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204595</th>\n",
              "      <td>힙합을</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204596</th>\n",
              "      <td>힙합의</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204597</th>\n",
              "      <td>힙합이</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204598</th>\n",
              "      <td>힙힙힙합</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>204599 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0\n",
              "0          가가\n",
              "1       가가가만히\n",
              "2         가거든\n",
              "3        가거들랑\n",
              "4         가거라\n",
              "...       ...\n",
              "204594    힙합은\n",
              "204595    힙합을\n",
              "204596    힙합의\n",
              "204597    힙합이\n",
              "204598   힙힙힙합\n",
              "\n",
              "[204599 rows x 1 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_csv(f\"{root_path}/ballad_ward.txt\", encoding=\"cp949\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "21sZEJqSxiN6",
        "outputId": "6df2c1b9-fe9f-40c1-f8df-78bdb8df1435"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83137    박동소리\n",
              "dtype: object"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words[words.str.startswith(\"박\")].sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEEJSzAQ1ovf",
        "outputId": "c5e01d3d-09cb-41a9-b8f2-d94577ce487d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(str, '한걸음이')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word = words[words.str.startswith(\"한\")].sample(1).values[0]\n",
        "type(word), word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cswGVrK0vlbJ"
      },
      "source": [
        "### 3행시\n",
        "- 첫번째 음절에 맞는 word 를 찾아서 넣어주고, generate 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9V2h3JEAkrg"
      },
      "outputs": [],
      "source": [
        "def force_poem(input_letter):\n",
        "    # 두음 법칙 사전\n",
        "    dooeum = {\"라\":\"나\", \"락\":\"낙\", \"란\":\"난\", \"랄\":\"날\", \"람\":\"남\", \"랍\":\"납\", \"랑\":\"낭\", \n",
        "          \"래\":\"내\", \"랭\":\"냉\", \"냑\":\"약\", \"략\":\"약\", \"냥\":\"양\", \"량\":\"양\", \"녀\":\"여\", \n",
        "          \"려\":\"여\", \"녁\":\"역\", \"력\":\"역\", \"년\":\"연\", \"련\":\"연\", \"녈\":\"열\", \"렬\":\"열\", \n",
        "          \"념\":\"염\", \"렴\":\"염\", \"렵\":\"엽\", \"녕\":\"영\", \"령\":\"영\", \"녜\":\"예\", \"례\":\"예\", \n",
        "          \"로\":\"노\", \"록\":\"녹\", \"론\":\"논\", \"롱\":\"농\", \"뢰\":\"뇌\", \"뇨\":\"요\", \"료\":\"요\", \n",
        "          \"룡\":\"용\", \"루\":\"누\", \"뉴\":\"유\", \"류\":\"유\", \"뉵\":\"육\", \"륙\":\"육\", \"륜\":\"윤\", \n",
        "          \"률\":\"율\", \"륭\":\"융\", \"륵\":\"늑\", \"름\":\"늠\", \"릉\":\"능\", \"니\":\"이\", \"리\":\"이\", \n",
        "          \"린\":'인', '림':'임', '립':'입'}\n",
        "    # 결과물을 담을 list\n",
        "    res_l = []\n",
        "    len_sequence = 0\n",
        "\n",
        "    # 한 글자씩 인덱스와 함께 가져옴\n",
        "    for idx, val in enumerate(input_letter):\n",
        "        # 두음 법칙 적용\n",
        "        if val in dooeum.keys():\n",
        "            val = dooeum[val]\n",
        "\n",
        "        # 발라드에 있는 단어 적용\n",
        "        try:\n",
        "            word = words[words.str.startswith(val)].sample(1).values[0]\n",
        "        except:\n",
        "            word = val\n",
        "        \n",
        "        # 좀더 매끄러운 삼행시를 위해 이전 문장이랑 현재 음절 연결\n",
        "        # 이후 generate 된 문장에서 이전 문장에 대한 데이터 제거\n",
        "        link_with_pre_sentence = (\" \".join(res_l)+ \" \" + word + \" \" if idx != 0 else word).strip()\n",
        "        # print(link_with_pre_sentence)\n",
        "\n",
        "        # 연결된 문장을 인코딩\n",
        "        input_ids = tokenizer.encode(link_with_pre_sentence, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "        # 인코딩 값으로 문장 생성\n",
        "        output_sequence = model.generate(\n",
        "            input_ids=input_ids.to(device), \n",
        "            do_sample=True,\n",
        "            max_length=42,\n",
        "            min_length=len_sequence + 2,\n",
        "            temperature=0.9,\n",
        "            repetition_penalty=1.5,\n",
        "            no_repeat_ngram_size=2)\n",
        "\n",
        "        # 생성된 문장 리스트로 변환 (인코딩 되어있고, 생성된 문장 뒤로 padding 이 있는 상태)\n",
        "        generated_sequence = output_sequence.tolist()[0]\n",
        "\n",
        "        # padding index 앞까지 slicing 함으로써 padding 제거, padding이 없을 수도 있기 때문에 조건문 확인 후 제거\n",
        "        # 사용할 generated_sequence 가 5보다 짧으면 강제적으로 길이를 8로 해준다... \n",
        "        if tokenizer.pad_token_id in generated_sequence:\n",
        "            check_index = generated_sequence.index(tokenizer.pad_token_id)\n",
        "            check_index = check_index if check_index-len_sequence > 3 else len_sequence + 8\n",
        "            generated_sequence = generated_sequence[:check_index]\n",
        "\n",
        "        \n",
        "        # 인코딩된 word 를 기준으로 slicing 해준다. \n",
        "        word_encode = tokenizer.encode(word, add_special_tokens=False, return_tensors=\"pt\").tolist()[0][0]\n",
        "        split_index = len(generated_sequence) - 1 - generated_sequence[::-1].index(word_encode)\n",
        "        \n",
        "        generated_sequence = generated_sequence[split_index:]\n",
        "        \n",
        "        # print(tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True))\n",
        "        # 다음 음절을 위해 길이 갱신\n",
        "        len_sequence += len([elem for elem in generated_sequence if elem not in(tokenizer.all_special_ids)])        \n",
        "        # 결과물 디코딩\n",
        "        decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
        "\n",
        "        # 결과물 리스트에 담기\n",
        "        res_l.append(decoded_sequence)\n",
        "\n",
        "\n",
        "    # 결과물 list에서 한 줄씩 출력\n",
        "    for letter, res in zip(input_letter, res_l):\n",
        "        print(f\"{letter} :\", res)"
      ]
    }
  ]
}